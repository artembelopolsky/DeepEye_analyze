{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0db8a2",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0644c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Enable interactive Matplotlib plots in the notebook\n",
    "%matplotlib qt5\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import astropy.convolution as krn\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Button\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc4037",
   "metadata": {},
   "source": [
    "# Interactive Plotting Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c385bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for interactive 2D plot\n",
    "\n",
    "# Create a custom class to handle button clicks\n",
    "class PlotUpdater:\n",
    "    def __init__(self, ax, df, plot_type='2D'):\n",
    "        self.ax = ax\n",
    "        self.df = df\n",
    "        self.plot_type = plot_type\n",
    "        self.current_index = 0\n",
    "        self.button_next = Button(ax, 'Next', color='lightgoldenrodyellow', hovercolor='0.975')\n",
    "        self.button_prev = Button(ax, 'Previous', color='lightgoldenrodyellow', hovercolor='0.975')\n",
    "        self.button_next.on_clicked(self.next_click)\n",
    "        self.button_prev.on_clicked(self.prev_click)        \n",
    "        \n",
    "        # Set text color for the buttons to black\n",
    "        self.button_next.label.set_color('black')\n",
    "        self.button_prev.label.set_color('black')\n",
    "        \n",
    "    def next_click(self, event):\n",
    "        if self.current_index < len(self.df.trialNr.unique()) - 1:\n",
    "            self.current_index += 1\n",
    "            self.update_plot()\n",
    "        else:\n",
    "            self.current_index = self.current_index - len(self.df.trialNr.unique()) # if at the end of df, wrap over to start of df\n",
    "            self.update_plot()\n",
    "\n",
    "    def prev_click(self, event):\n",
    "        if self.current_index > 0:\n",
    "            self.current_index -= 1\n",
    "            self.update_plot()\n",
    "        else:\n",
    "            self.current_index = self.current_index + len(self.df.trialNr.unique())-1 # if at the start of df, wrap over to end of df\n",
    "            self.update_plot()\n",
    "\n",
    "    def update_plot(self):\n",
    "        self.ax.clear()               \n",
    "        \n",
    "        # select a trial based on current_index\n",
    "        current_trial = self.df.trialNr.unique()[self.current_index]\n",
    "        df = self.df[self.df.trialNr == current_trial]\n",
    "        \n",
    "        if self.plot_type == '2D':        \n",
    "            # plot 2D graph       \n",
    "            self.plot2D(df, current_trial = current_trial)\n",
    "        else:        \n",
    "            # plot 1D graph\n",
    "            self.plot1D(df, current_trial = current_trial)       \n",
    "\n",
    "        \n",
    "    def plot2D(self, df, current_trial):\n",
    "        \n",
    "\n",
    "        # plot raw samples\n",
    "        self.ax.scatter(df['user_pred_px_x'], df['user_pred_px_y'], c='orange', alpha=0.5, edgecolors='black', label='raw samples')\n",
    "\n",
    "        # plot fixations and remove no fixations/saccades (zeros)\n",
    "        self.ax.scatter(df['FixXPos'][df['FixXPos']>0], df['FixYPos'][df['FixYPos']>0], c='blue', alpha=0.5, edgecolors='black', label='fixations')\n",
    "        \n",
    "#         # plot target and fixation cirle\n",
    "#         self.ax.scatter(df.fixationStimX, df.fixationStimY, c='red')\n",
    "#         self.ax.scatter(df.targetX, df.fixationStimY, c='green')\n",
    "        \n",
    "#         # plot target and fixation vertical lines\n",
    "#         self.ax.plot(np.ones(df.resY.iloc[0].astype('int')) * df.fixationStimX.iloc[0], np.arange(df.resY.iloc[0]), c='red', lw=1, linestyle='dashed')\n",
    "#         self.ax.plot(np.ones(df.resY.iloc[0].astype('int')) * df.targetX.iloc[0], np.arange(df.resY.iloc[0]), c='green', lw=1, linestyle='dashed')\n",
    "\n",
    "        # set axis limits based on screen resolution\n",
    "        self.ax.set_xlim((0, df.resX.iloc[0]))\n",
    "        self.ax.set_ylim((df.resY.iloc[0]), 0)\n",
    "        \n",
    "        # label the axes\n",
    "        self.ax.set_xlabel('Horizontal eye position (pixels)')\n",
    "        self.ax.set_ylabel('Vertical eye position (pixels)')\n",
    "        \n",
    "        self.ax.legend()\n",
    "        self.ax.set_title(f'Target {df.locStudiedImage.iloc[0]}, Trial {current_trial}')    \n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        plt.draw()\n",
    "        \n",
    "   \n",
    "\n",
    "    def plot1D(self, df, current_trial):  \n",
    "                     \n",
    "        # rescale time variable to start from zero\n",
    "        t = np.array(df.sampTime)\n",
    "        t = t-t[0]\n",
    "        \n",
    "               \n",
    "        # plot raw points and fixations\n",
    "        self.ax.scatter(t, df.user_pred_px_x, c='orange', alpha=0.5, edgecolors='black', label='raw_samples')\n",
    "        self.ax.scatter(t, df.FixXPos, c='blue', alpha=0.5, edgecolors='black', label='fixations')\n",
    "        \n",
    "        # add target and fixaton horizontal lines\n",
    "        self.ax.plot(t, df.targetX, c='green', lw=1)\n",
    "        self.ax.plot(t, df.fixationStimX, c='red', lw=1, linestyle='dashed')\n",
    "        \n",
    "        # label the axes\n",
    "        self.ax.set_xlabel('Time (ms)')\n",
    "        self.ax.set_ylabel('Horizontal eye position (pixels)')\n",
    "        \n",
    "        # set axis limits based on horizantal resolution\n",
    "        self.ax.set_ylim((df.resX.iloc[0]), 0)\n",
    "        \n",
    "        # plot saccade latency as a horizontal line\n",
    "        self.ax.plot(np.array([df.SaccLat.iloc[0]] * df.resX.iloc[0].astype(int)), np.arange(df.resX.iloc[0]), c='black', lw=1)\n",
    "        \n",
    "        \n",
    "        # set the rest\n",
    "        self.ax.set_title(f'Target {df.target.iloc[0]}, Trial {current_trial}')\n",
    "        self.ax.legend()\n",
    "        self.ax.grid(True)\n",
    "        \n",
    "        plt.draw()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b070fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHeat(screenRes, xPos, yPos):\n",
    "        xMax = screenRes[0]\n",
    "        yMax = screenRes[1]\n",
    "        xMin = 0\n",
    "        yMin = 0\n",
    "        kernelPar = 50\n",
    "\n",
    "        # Input handeling\n",
    "        xlim = np.logical_and(xPos < xMax, xPos > xMin)\n",
    "        ylim = np.logical_and(yPos < yMax, yPos > yMin)\n",
    "        xyLim = np.logical_and(xlim, ylim)\n",
    "        dataX = xPos[xyLim]\n",
    "        dataX = np.floor(dataX)\n",
    "        dataY = yPos[xyLim]\n",
    "        dataY = np.floor(dataY)\n",
    "\n",
    "        # initiate map and gauskernel\n",
    "        gazeMap = np.zeros([int((xMax-xMin)),int((yMax-yMin))])+0.0001\n",
    "        gausKernel = krn.Gaussian2DKernel(kernelPar)\n",
    "\n",
    "        # Rescale the position vectors (if xmin or ymin != 0)\n",
    "        dataX -= xMin\n",
    "        dataY -= yMin\n",
    "\n",
    "        # Now extract all the unique positions and number of samples\n",
    "        xy = np.vstack((dataX, dataY)).T\n",
    "        uniqueXY, idx, counts = uniqueRows(xy)\n",
    "        uniqueXY = uniqueXY.astype(int)\n",
    "        # populate the gazeMap\n",
    "        gazeMap[uniqueXY[:,0], uniqueXY[:,1]] = counts\n",
    "\n",
    "        # Convolve the gaze with the gauskernel\n",
    "        heatMap = np.transpose(krn.convolve_fft(gazeMap,gausKernel))\n",
    "        heatMap = heatMap/np.max(heatMap)\n",
    "\n",
    "        return heatMap\n",
    "\n",
    "def uniqueRows(x):\n",
    "    y = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, idx, counts = np.unique(y, return_index=True, return_counts = True)\n",
    "    uniques = x[idx]\n",
    "    return uniques, idx, counts\n",
    "\n",
    "\n",
    "def np_euclidean_distance(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sqrt(np.sum(np.square(y_pred - y_true), axis=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b54495",
   "metadata": {},
   "source": [
    "# Preprocess, extract fixations and add them to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215be8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "def extract_fixations(df, path):\n",
    "    \n",
    "    sys.path.append('./FixationDetection')\n",
    "    from I2MC import runI2MC\n",
    "    \n",
    "    # order frames and drop duplicate samples (with same sampleTime)\n",
    "    df = df[df.fName.notna()]\n",
    "    df.frameNr = df.frameNr.apply(pd.to_numeric, errors='coerce') # if framerNr is not a number, it is replaces with nan\n",
    "    df = df[df.frameNr.notna()] # filter out rows where frameNr is a nan\n",
    "\n",
    "    df = df[df.sampTime.notna()]\n",
    "    df = df[df.user_pred_px_x.notna()]\n",
    "    df = df[df.user_pred_px_y.notna()]\n",
    "    df = df.apply(pd.to_numeric, errors='ignore') # if str convert str to numbers\n",
    "\n",
    "    df = df.sort_values('frameNr')\n",
    "    df = df.reset_index(drop=True)\n",
    "    # df = df.drop_duplicates(subset=['user_pred_px_x', 'user_pred_px_y'], ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=['sampTime'], ignore_index=True)\n",
    "       \n",
    "    \n",
    "    # get fixations for the original datafile for each participant\n",
    "    fixDF = runI2MC(path, plotData = False)\n",
    "\n",
    "    # add extracted fixations to the original data file (two new columns)\n",
    "    # for each timestamp where fixation was detected, FixXPos and FixYPos are added\n",
    "    idx = 0 # index of fixDF\n",
    "    FixXPos = np.zeros(df.shape[0])\n",
    "    FixYPos = np.zeros(df.shape[0])\n",
    "    FixStartEnd = np.empty(df.shape[0], dtype='U10')\n",
    "    FixStartEnd.fill('') # explicitly fill the array (good practice)\n",
    "    FixDur = np.zeros(df.shape[0])\n",
    "\n",
    "    DistFromPrevFix = np.zeros(df.shape[0])\n",
    "    PrevFixXPos = np.zeros(df.shape[0])\n",
    "    PrevFixYPos = np.zeros(df.shape[0])\n",
    "    prev_fix_x = False # keep track of xy when fixation ends\n",
    "    prev_fix_y = False\n",
    "\n",
    "    PrevFixSampTime = np.zeros(df.shape[0])\n",
    "    prev_fix_sampTime = 0\n",
    "\n",
    "    # iterate thru the original dataframe, thru each sample\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # make sure not to iterate out of range\n",
    "        if idx < fixDF.shape[0]:\n",
    "\n",
    "            # go to next fixation when fixation ends\n",
    "            if row['sampTime'] > np.array(fixDF.FixEnd)[idx]:\n",
    "                    idx += 1\n",
    "\n",
    "            # make sure not to iterate out of range\n",
    "            if idx < fixDF.shape[0]:\n",
    "\n",
    "                # when samples are within fixation, accumulate FixXPos and FixYPos\n",
    "                if row['sampTime'] >= np.array(fixDF.FixStart)[idx] and row['sampTime'] <= np.array(fixDF.FixEnd)[idx]:\n",
    "\n",
    "                    FixXPos[index] = (np.array(fixDF.XPos)[idx])\n",
    "                    FixYPos[index] = (np.array(fixDF.YPos)[idx])\n",
    "\n",
    "                # label samples on which fixation starts and ends\n",
    "                if row['sampTime'] == np.array(fixDF.FixStart)[idx]:             \n",
    "                    FixStartEnd[index] = 'fix_start'\n",
    "\n",
    "                    if prev_fix_x != False:\n",
    "\n",
    "                        PrevFixXPos[index] = prev_fix_x\n",
    "                        PrevFixYPos[index] = prev_fix_y\n",
    "\n",
    "                        DistFromPrevFix[index] = np.sqrt((np.array(fixDF.XPos)[idx] - prev_fix_x)**2 \n",
    "                                                + (np.array(fixDF.YPos)[idx] - prev_fix_y)**2)\n",
    "                        PrevFixSampTime[index] = prev_fix_sampTime\n",
    "\n",
    "\n",
    "                elif row['sampTime'] == np.array(fixDF.FixEnd)[idx]:                \n",
    "                    FixStartEnd[index] = 'fix_end' \n",
    "                    FixDur[index] = np.array(fixDF.FixDur)[idx]\n",
    "\n",
    "                    prev_fix_x = np.array(fixDF.XPos)[idx]\n",
    "                    prev_fix_y = np.array(fixDF.YPos)[idx]\n",
    "                    prev_fix_sampTime = np.array(row['sampTime'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # add fixations to original dataframe\n",
    "    df['FixXPos'] = np.array(FixXPos)\n",
    "    df['FixYPos'] = np.array(FixYPos)\n",
    "    df['FixStartEnd'] = FixStartEnd\n",
    "    df['FixDur'] = np.array(FixDur)\n",
    "    df['DistFromPrevFix'] = DistFromPrevFix\n",
    "    df['PrevFixSampTime'] = PrevFixSampTime\n",
    "    df['PrevFixXPos'] = PrevFixXPos\n",
    "    df['PrevFixYPos'] = PrevFixYPos\n",
    "    \n",
    "    \n",
    "    # Remove all negative xs, ys\n",
    "    df = df[(df['FixXPos'] > 0) & (df['FixYPos'] > 0) & (df['user_pred_px_x'] > 0) & (df['user_pred_px_y'] > 0)]\n",
    "\n",
    "\n",
    "    # Save the pre-processed dataframe\n",
    "    df.to_csv((os.path.splitext(path)[0] + '_extra.csv'), index=False)  \n",
    "\n",
    "\n",
    "    # Extract only samples when the target was presented\n",
    "    df = df[df.event=='target_on']\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "# # # Label trials with too few data points\n",
    "# # a = df.groupby('trialNr').count().reset_index()\n",
    "# # a = a[['trialNr', 'sampTime']]\n",
    "# # # 3) rename the columns so they would be added\n",
    "# # a.columns = ['trialNr', 'samplesPerTrial']\n",
    "# # df = pd.merge(df, a, on=\"trialNr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ea296",
   "metadata": {},
   "source": [
    "# 2D plot of fixations and raw samples\n",
    "\n",
    "## 1. plot all raw x,y\n",
    "## 2. plot all fixations x,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05980c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2d(df, subj_nr, path, bboxes=False, box_w=False, box_h=False, stimuli=False):\n",
    "    \n",
    "    import matplotlib.patches as patches\n",
    "    import ast\n",
    "    import matplotlib.image as mpimg\n",
    "\n",
    "    conditions = ['bags', 'hats', 'sunglasses', 'shoes', 'dresses']\n",
    "\n",
    "\n",
    "    for cond in conditions:\n",
    "\n",
    "        plt.figure()\n",
    "        plt.style.use('ggplot')\n",
    "        plt.title(f'{cond}, #{subj_nr}')\n",
    "\n",
    "        # select condition\n",
    "        a = df[df.stimuli==cond]  \n",
    "\n",
    "        raw_h = plt.scatter(a.user_pred_px_x, a.user_pred_px_y, c='orange', alpha=0.5, edgecolors='black')\n",
    "\n",
    "        # remove no fixations/saccades (zeros)\n",
    "        \n",
    "        fix_h = plt.scatter(a.FixXPos[a.FixXPos>0], a.FixYPos[a.FixYPos>0], c='blue', alpha=0.5, edgecolors='black') \n",
    "\n",
    "        # plot center fixation dot\n",
    "        plt.scatter(a.resX.iloc[0]/2, a.resY.iloc[0]/2, c='red')      \n",
    "              \n",
    "        \n",
    "        if bboxes:\n",
    "            # plot image boxes\n",
    "            if cond == 'dresses':\n",
    "                height = 375\n",
    "            else:\n",
    "                width = 250\n",
    "                height = 250\n",
    "            \n",
    "            # get coordinates for the first entry\n",
    "            left_top_coords = a.imageLocations.iloc[0]\n",
    "            \n",
    "            # get paths to image names for that frame\n",
    "            imageNames = a.imageNames.iloc[0]\n",
    "            \n",
    "            # Convert string to actual list\n",
    "            left_top_coords = ast.literal_eval(left_top_coords)\n",
    "            print(f'coordinates: {left_top_coords}')\n",
    "            print(f'imageNames: {imageNames}')\n",
    "            imageNames_paths = ast.literal_eval(imageNames)\n",
    "            \n",
    "            if stimuli:\n",
    "                print('') # draw stimuli images\n",
    "            \n",
    "            for stim_idx, coord in enumerate(left_top_coords):                \n",
    "                left = float(coord[0])\n",
    "                top = float(coord[1])            \n",
    "               \n",
    "                rect = patches.Rectangle((left,top),width,height, \n",
    "                                    fill = False,\n",
    "                                    color = \"purple\",\n",
    "                                    linewidth = 2)\n",
    "                \n",
    "                plt.gca().add_patch(rect)\n",
    "                \n",
    "                if stimuli:\n",
    "                    print('Plotting stimuli images') # draw stimuli images\n",
    "                    \n",
    "                    # Remove the last directory from the path\n",
    "                    new_path = os.path.dirname(path)\n",
    "                    \n",
    "                    # Fix image name if ends on 'png'\n",
    "                    if imageNames_paths[stim_idx][-3:] == 'png':\n",
    "                        imageNames_paths[stim_idx] = imageNames_paths[stim_idx].strip('png') + 'jpg'\n",
    "                    \n",
    "                    # Join the main path with path to stimuli\n",
    "                    new_path = new_path + imageNames_paths[stim_idx][1:]\n",
    "                    \n",
    "                    # Load the images\n",
    "                    image = mpimg.imread(new_path)\n",
    "\n",
    "                    # Define the extent (left, right, bottom, top) in data coordinates\n",
    "                    extent = [left, left+width, top+height, top]\n",
    "                    \n",
    "                    # Plot the image at specific coordinates\n",
    "                    plt.imshow(image, extent=extent)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "        plt.xlim((0, df.resX.iloc[0]))\n",
    "        plt.ylim((df.resY.iloc[0]), 0)\n",
    "\n",
    "        plt.xlabel('Horizontal eye position (pixels)')\n",
    "        plt.ylabel('Vertical eye position (pixels)')\n",
    "\n",
    "        plt.legend((raw_h, fix_h), ('raw samples', 'fixations'), scatterpoints=1)\n",
    "\n",
    "        # save figure\n",
    "        plt.savefig(os.path.join(path, subj_nr+f'_2D{cond}.jpg'), dpi=1000, pad_inches=0)\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9eacc",
   "metadata": {},
   "source": [
    "# Calculate novelty index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def novelty_index(df, fn):\n",
    "\n",
    "    # select condition\n",
    "    testPhase_df = df[df.phase=='test']\n",
    "    # testPhase_df =  testPhase_df.drop_duplicates(subset=['FixXPos', 'FixYPos'], ignore_index=True) # one unique fixation per row\n",
    "    testPhase_df = testPhase_df[testPhase_df.FixStartEnd=='fix_end']\n",
    "\n",
    "    screen_centerX = testPhase_df.resX.iloc[0]/2\n",
    "    distFixToImageBoarder = testPhase_df.distBetweenImages.iloc[0]/2 - 480/2\n",
    "\n",
    "    # filter out fixations not reaching the image box\n",
    "    testPhase_df = testPhase_df[((testPhase_df.FixXPos < (screen_centerX-distFixToImageBoarder))\n",
    "                                         | (testPhase_df.FixXPos > (screen_centerX+distFixToImageBoarder)))]\n",
    "\n",
    "    # label the fixation on left or right side                                     \n",
    "    testPhase_df['FixatedImage'] = np.where(testPhase_df.FixXPos < screen_centerX, 'left', 'right')\n",
    "    testPhase_df['FixatedNovel'] = np.where(testPhase_df.FixatedImage == testPhase_df.locStudiedImage, 'old', 'novel')\n",
    "\n",
    "\n",
    "    novelty_idx_fix = []\n",
    "    novelty_idx_fixDur = []\n",
    "    # iterate through trials\n",
    "    for i, group in testPhase_df.groupby('trialNr'):\n",
    "\n",
    "        # calculate the proportion of novel fixations on each trial\n",
    "        novelty_idx_fix.append(group.FixatedNovel[group.FixatedNovel=='novel'].count() / group.FixatedNovel.count())\n",
    "\n",
    "        # calculate the proportion of novel fixationTime on each trial\n",
    "        novelty_idx_fixDur.append(group.FixDur[group.FixatedNovel=='novel'].sum() / group.FixDur.sum())\n",
    "\n",
    "    novelty_idx_fix = np.array(novelty_idx_fix)\n",
    "    novelty_idx_fixDur = np.array(novelty_idx_fixDur)\n",
    "\n",
    "    # Prepare output df\n",
    "    output_df = testPhase_df.drop_duplicates(subset=['trialNr'], ignore_index=True) # one trial per row\n",
    "    output_df = output_df.drop(['frameNr','sampTime', 'user_pred_px_x', 'user_pred_px_y'], axis=1) # drop columns by name\n",
    "    output_df = output_df.iloc[:,:20] # drop columns by index\n",
    "\n",
    "    # Log the novelty indices per trial\n",
    "    output_df['noveltyIdx_fixCountProp'] = novelty_idx_fix\n",
    "    output_df['noveltyIdx_fixDurProp'] = novelty_idx_fixDur\n",
    "    \n",
    "    # Add subject number based on deepeye id\n",
    "    output_df['deepeye-id'] = fn\n",
    "\n",
    "    # Save the output file\n",
    "    # output_df.to_csv(os.path.join(path_to_folders, 'analysis', fn+'_analyzed.csv'))\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585816e",
   "metadata": {},
   "source": [
    "# Calculating AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ecb76d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point (952.6676381614304, 599.4072538897838) belongs to bounding box: None_bags, image:None_bags\n",
      "The point (948.2519074146974, 767.7651375770661) belongs to bounding box: 4, image:bag1.jpg\n",
      "The point (387.8911822181868, 326.2131742771812) belongs to bounding box: 0, image:bag3.jpg\n",
      "The point (1295.2049488565362, 173.9316564939054) belongs to bounding box: None_bags, image:None_bags\n",
      "The point (937.9683261094126, 231.7050931888765) belongs to bounding box: 1, image:bag4.jpg\n",
      "The point (1488.4406476955348, 161.7744566369664) belongs to bounding box: 2, image:bag2.jpg\n",
      "The point (976.2063739243348, 870.6328315490952) belongs to bounding box: 4, image:bag1.jpg\n",
      "The point (386.94852770977377, 837.9078482342677) belongs to bounding box: 3, image:bag5.jpg\n",
      "The point (937.672592467625, 264.2535755268076) belongs to bounding box: 1, image:bag4.jpg\n",
      "The point (377.1069464722961, 304.4551228750617) belongs to bounding box: 0, image:bag3.jpg\n",
      "The point (1293.9746693218694, 157.31492734030297) belongs to bounding box: None_bags, image:None_bags\n",
      "The point (1473.008548154858, 144.0409623828385) belongs to bounding box: 2, image:bag2.jpg\n",
      "The point (1478.2482968973222, 882.2284573193828) belongs to bounding box: 5, image:bag6.jpg\n",
      "The point (1495.3172983258837, 802.724441480686) belongs to bounding box: 5, image:bag6.jpg\n",
      "The point (936.0744986282084, 235.51879508527833) belongs to bounding box: 1, image:bag4.jpg\n",
      "The point (929.6708724037029, 869.6325633875124) belongs to bounding box: 4, image:bag1.jpg\n",
      "The point (367.9878404163868, 819.4723755663844) belongs to bounding box: 3, image:bag5.jpg\n",
      "The point (378.6794057056418, 325.49097965315303) belongs to bounding box: 0, image:bag3.jpg\n",
      "The point (1259.6658669923331, 116.1115414027755) belongs to bounding box: None_bags, image:None_bags\n",
      "The point (1451.5111910112396, 150.8411835633131) belongs to bounding box: 2, image:bag2.jpg\n",
      "The point (970.3894424227552, 640.6351980204707) belongs to bounding box: 4, image:hat4.jpeg\n",
      "The point (573.5412469374743, 361.11803425764947) belongs to bounding box: None_hats, image:None_hats\n",
      "The point (971.6132874985999, 829.4940561407045) belongs to bounding box: 4, image:hat4.jpeg\n",
      "The point (985.0807482387543, 546.595207344611) belongs to bounding box: None_hats, image:None_hats\n",
      "The point (498.9398687691929, 721.3311644303072) belongs to bounding box: 3, image:hat2.jpg\n",
      "The point (1000.1536090428192, 279.474291242871) belongs to bounding box: 1, image:hat3.jpg\n",
      "The point (1545.2352994929072, 168.01131901529743) belongs to bounding box: 2, image:hat5.jpeg\n",
      "The point (1507.3017921763749, 778.1076588319477) belongs to bounding box: 5, image:hat1.jpg\n",
      "The point (1389.5099487273142, 724.7679210686885) belongs to bounding box: 5, image:hat1.jpg\n",
      "The point (984.1370238452, 314.67060607000093) belongs to bounding box: 1, image:hat3.jpg\n",
      "The point (961.5917898937453, 879.5686891249611) belongs to bounding box: 4, image:hat4.jpeg\n",
      "The point (374.065823935172, 924.531991124918) belongs to bounding box: None_hats, image:None_hats\n",
      "The point (387.82617993882815, 887.1344789835191) belongs to bounding box: None_hats, image:None_hats\n",
      "The point (547.523167130696, 766.0398342076112) belongs to bounding box: 3, image:hat2.jpg\n",
      "The point (1535.207801236455, 155.83480162748586) belongs to bounding box: 2, image:hat5.jpeg\n",
      "The point (998.4739355032777, 493.4068852577865) belongs to bounding box: None_hats, image:None_hats\n",
      "The point (998.4739355032777, 493.4068852577865) belongs to bounding box: None_shoes, image:None_shoes\n",
      "The point (962.7849010690014, 294.6774860583213) belongs to bounding box: 1, image:shoe4.jpg\n",
      "The point (919.6618821779145, 867.0177872578986) belongs to bounding box: 4, image:shoe5.jpg\n",
      "The point (1371.8572688110553, 130.4586391362419) belongs to bounding box: 2, image:shoe6.jpg\n",
      "The point (1510.6006319567798, 156.72205013111375) belongs to bounding box: 2, image:shoe6.jpg\n",
      "The point (1570.0667443995933, 182.4125512734809) belongs to bounding box: None_shoes, image:None_shoes\n",
      "The point (401.8689298280077, 405.6034156669497) belongs to bounding box: None_shoes, image:None_shoes\n",
      "The point (389.9805478434724, 878.4904233059306) belongs to bounding box: 3, image:shoe1.jpg\n",
      "The point (981.3059421243746, 846.8134702263617) belongs to bounding box: 4, image:shoe5.jpg\n",
      "The point (1529.574160765501, 665.2312719071393) belongs to bounding box: 5, image:shoe3.jpg\n",
      "The point (1528.0317002383254, 750.296631827523) belongs to bounding box: 5, image:shoe3.jpg\n",
      "The point (1585.7586830857185, 159.3081658523891) belongs to bounding box: None_shoes, image:None_shoes\n",
      "The point (950.2955012118216, 308.249563316519) belongs to bounding box: 1, image:shoe4.jpg\n",
      "The point (993.7621750883525, 300.61875236327944) belongs to bounding box: 1, image:shoe4.jpg\n",
      "The point (349.2638527607282, 371.269063289475) belongs to bounding box: None_shoes, image:None_shoes\n",
      "The point (963.9988493129632, 573.7404234563288) belongs to bounding box: None_sunglasses, image:None_sunglasses\n",
      "The point (926.7787260352418, 236.36565105922924) belongs to bounding box: 1, image:sunglasses2.jpg\n",
      "The point (946.0443551341326, 854.9806903477275) belongs to bounding box: 4, image:sunglasses3.jpeg\n",
      "The point (1452.6723581862566, 683.4769139149146) belongs to bounding box: 5, image:sunglasses5.jpg\n",
      "The point (1229.711243679645, 201.5844609448505) belongs to bounding box: None_sunglasses, image:None_sunglasses\n",
      "The point (365.6112976321167, 334.3935322007075) belongs to bounding box: 0, image:sunglasses1.jpg\n",
      "The point (299.7558117230091, 782.2560045563572) belongs to bounding box: None_sunglasses, image:None_sunglasses\n",
      "The point (329.3402825878603, 828.1186911863729) belongs to bounding box: None_sunglasses, image:None_sunglasses\n",
      "The point (925.3760885639219, 200.1904752756572) belongs to bounding box: 1, image:sunglasses2.jpg\n",
      "The point (924.0069992041051, 852.2768415857602) belongs to bounding box: 4, image:sunglasses3.jpeg\n",
      "The point (1005.6539404264172, 832.370852034247) belongs to bounding box: 4, image:sunglasses3.jpeg\n",
      "The point (1535.877689504046, 113.03072878535713) belongs to bounding box: 2, image:sunglasses6.jpg\n",
      "The point (1442.39379198231, 710.2010925069335) belongs to bounding box: 5, image:sunglasses5.jpg\n",
      "The point (959.2876237539256, 815.0576203357135) belongs to bounding box: 4, image:Model_XL_Dress_6.png\n",
      "The point (1515.174789942735, 678.3486051771407) belongs to bounding box: 5, image:Model_S2_Dress_4.png\n",
      "The point (1527.5426891246789, 128.35183360527452) belongs to bounding box: 2, image:Model_S3_Dress_3.png\n",
      "The point (908.8284016104597, 244.42548069386885) belongs to bounding box: 1, image:Model_M_Dress_1.png\n",
      "The point (387.263499462817, 347.5894020142457) belongs to bounding box: 0, image:Model_S1_Dress_2.png\n",
      "The point (336.7518319368689, 878.4586580763365) belongs to bounding box: None_dresses, image:None_dresses\n",
      "The point (937.0992212779862, 302.09853852001015) belongs to bounding box: 1, image:Model_M_Dress_1.png\n",
      "The point (1498.7670616931478, 843.0443983975081) belongs to bounding box: 5, image:Model_S2_Dress_4.png\n",
      "The point (1562.9520993975575, 146.1872441154464) belongs to bounding box: 2, image:Model_S3_Dress_3.png\n",
      "The point (959.1320847284189, 851.4084310724245) belongs to bounding box: 4, image:Model_XL_Dress_6.png\n",
      "The point (366.5949329829584, 912.1457529797957) belongs to bounding box: 3, image:Model_L_Dress_5.png\n",
      "The point (934.8162303187447, 269.6028098323331) belongs to bounding box: 1, image:Model_M_Dress_1.png\n"
     ]
    }
   ],
   "source": [
    "def addAOI(df):  \n",
    "    \n",
    "    def is_point_in_box(point, box):\n",
    "        \"\"\"\n",
    "        Determine if a point is within a bounding box.\n",
    "\n",
    "        Parameters:\n",
    "        - point: A tuple (x, y) representing the point.\n",
    "        - box: A tuple ((x1, y1), (x2, y2)) representing the bounding box, \n",
    "               where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner.\n",
    "\n",
    "        Returns:\n",
    "        - True if the point is within the box, False otherwise.\n",
    "        \"\"\"\n",
    "        px, py = point\n",
    "        (x1, y1), (x2, y2) = box\n",
    "\n",
    "        return x1 <= px <= x2 and y1 <= py <= y2\n",
    "\n",
    "    def get_bounding_box_assignment(boxes, point):\n",
    "        \"\"\"\n",
    "        Determine the bounding box a point belongs to.\n",
    "\n",
    "        Parameters:\n",
    "        - boxes: A list of tuples representing the bounding boxes.\n",
    "                 Each bounding box is defined as ((x1, y1), (x2, y2)).\n",
    "        - point: A tuple (x, y) representing the point.\n",
    "\n",
    "        Returns:\n",
    "        - The index of the bounding box the point belongs to, or None if it doesn't belong to any boxes.\n",
    "        \"\"\"\n",
    "        for i, box in enumerate(boxes):\n",
    "            if is_point_in_box(point, box):\n",
    "                return i\n",
    "        \n",
    "        return 'None'\n",
    "\n",
    "\n",
    "    import ast\n",
    "    width = 250\n",
    "    height = 250\n",
    "    imageName = 'None'\n",
    "    bbox_assignments = []\n",
    "    stim_assignments = []\n",
    "\n",
    "#     aoi_df = df[df.FixStartEnd=='fix_end']\n",
    "    aoi_df = df\n",
    "\n",
    "    for index, row in aoi_df.iterrows():\n",
    "        \n",
    "        if row.stimuli == 'dresses':\n",
    "            height = 375            \n",
    "        \n",
    "        # Get all stimuli names for this trial\n",
    "        imageNames = row.imageNames\n",
    "        \n",
    "        # Convert str to actual list\n",
    "        imageNames = ast.literal_eval(imageNames)\n",
    "\n",
    "        # Bounding boxes for this trial\n",
    "        bounding_boxes = []\n",
    "\n",
    "        # Get coordinates of stimuli for this fixation\n",
    "        left_top_coords = row.imageLocations\n",
    "\n",
    "        # Convert string to actual list\n",
    "        left_top_coords = ast.literal_eval(left_top_coords)\n",
    "\n",
    "        # Iterate over bboxes\n",
    "        for coord in left_top_coords:      \n",
    "            # Assemble coordinates for the bounding boxes\n",
    "            x1 = coord[0]\n",
    "            y1 = coord[1]\n",
    "            x2 = coord[0] + width\n",
    "            y2 = coord[1] + height\n",
    "            bounding_boxes.append([(x1,y1), (x2,y2)])        \n",
    "\n",
    "        # Get fixation coordinates\n",
    "        point = (row.FixXPos, row.FixYPos)\n",
    "        # get the index of the AOI where this fixation point falls\n",
    "        assignment = get_bounding_box_assignment(bounding_boxes, point)\n",
    "\n",
    "        # if fixation in the bounding box, get the stimulus name for this bounding box\n",
    "        if assignment != 'None':\n",
    "            imageName = imageNames[assignment].split('/')[-1]\n",
    "        else:\n",
    "            imageName = 'None_' + row.stimuli\n",
    "            assignment = assignment + '_' + row.stimuli\n",
    "\n",
    "\n",
    "        print(f'The point {point} belongs to bounding box: {assignment}, image:{imageName}')\n",
    "\n",
    "\n",
    "        # Accumulate assignements\n",
    "        bbox_assignments.append(assignment)\n",
    "        stim_assignments.append(imageName)\n",
    "\n",
    "    aoi_df['AOI_bbox'] = bbox_assignments\n",
    "    aoi_df['AOI_stim'] = stim_assignments    \n",
    "       \n",
    "    \n",
    "    return aoi_df\n",
    "\n",
    "a = addAOI(df_final2)\n",
    "a = a.reset_index(drop=True)\n",
    "a.to_csv(path +'/sample_participant_VBLExp1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6079f",
   "metadata": {},
   "source": [
    "# Get latency of all fixations and their order in a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490dba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: 0\n",
      "     frameNr                          fName  ...  FixLatency  FixationOrder\n",
      "0         18  2024_02_12_09_51_21_00018.jpg  ...         0.0            1.0\n",
      "22        46  2024_02_12_09_51_21_00046.jpg  ...       924.6            2.0\n",
      "35        66  2024_02_12_09_51_21_00066.jpg  ...      1584.5            3.0\n",
      "43        81  2024_02_12_09_51_21_00081.jpg  ...      2079.5            4.0\n",
      "48        87  2024_02_12_09_51_21_00087.jpg  ...      2276.8            5.0\n",
      "54        94  2024_02_12_09_51_21_00094.jpg  ...      2508.4            6.0\n",
      "64       115  2024_02_12_09_51_21_00115.jpg  ...      3200.9            7.0\n",
      "75       134  2024_02_12_09_51_21_00134.jpg  ...      3828.7            8.0\n",
      "86       156  2024_02_12_09_51_21_00156.jpg  ...      4554.6            9.0\n",
      "98       170  2024_02_12_09_51_21_00170.jpg  ...      5016.5           10.0\n",
      "112      186  2024_02_12_09_51_21_00186.jpg  ...      5545.1           11.0\n",
      "116      192  2024_02_12_09_51_21_00192.jpg  ...      5742.5           12.0\n",
      "137      215  2024_02_12_09_51_21_00215.jpg  ...      6500.8           13.0\n",
      "141      220  2024_02_12_09_51_21_00220.jpg  ...      6666.6           14.0\n",
      "153      251  2024_02_12_09_51_21_00251.jpg  ...      7689.6           15.0\n",
      "160      260  2024_02_12_09_51_21_00260.jpg  ...      7986.5           16.0\n",
      "177      279  2024_02_12_09_51_21_00279.jpg  ...      8613.5           17.0\n",
      "189      293  2024_02_12_09_51_21_00293.jpg  ...      9075.8           18.0\n",
      "199      305  2024_02_12_09_51_21_00305.jpg  ...      9471.6           19.0\n",
      "203      311  2024_02_12_09_51_21_00311.jpg  ...      9670.3           20.0\n",
      "\n",
      "[20 rows x 33 columns]\n",
      "\n",
      "Group: 1\n",
      "     frameNr                          fName  ...  FixLatency  FixationOrder\n",
      "217      348  2024_02_12_09_51_21_00348.jpg  ...       374.0            1.0\n",
      "232      365  2024_02_12_09_51_21_00365.jpg  ...       935.9            2.0\n",
      "241      375  2024_02_12_09_51_21_00375.jpg  ...      1266.8            3.0\n",
      "246      386  2024_02_12_09_51_21_00386.jpg  ...      1628.3            4.0\n",
      "250      392  2024_02_12_09_51_21_00392.jpg  ...      1826.5            5.0\n",
      "261      406  2024_02_12_09_51_21_00406.jpg  ...      2289.5            6.0\n",
      "282      429  2024_02_12_09_51_21_00429.jpg  ...      3047.2            7.0\n",
      "311      459  2024_02_12_09_51_21_00459.jpg  ...      4037.8            8.0\n",
      "317      469  2024_02_12_09_51_21_00469.jpg  ...      4367.5            9.0\n",
      "340      501  2024_02_12_09_51_21_00501.jpg  ...      5423.8           10.0\n",
      "353      519  2024_02_12_09_51_21_00519.jpg  ...      6017.0           11.0\n",
      "359      581  2024_02_12_09_51_21_00581.jpg  ...      8063.9           12.0\n",
      "373      596  2024_02_12_09_51_21_00596.jpg  ...      8558.3           13.0\n",
      "381      610  2024_02_12_09_51_21_00610.jpg  ...      9020.8           14.0\n",
      "392      622  2024_02_12_09_51_21_00622.jpg  ...      9416.4           15.0\n",
      "\n",
      "[15 rows x 33 columns]\n",
      "\n",
      "Group: 2\n",
      "     frameNr                          fName  ...  FixLatency  FixationOrder\n",
      "413      667  2024_02_12_09_51_21_00667.jpg  ...       345.5            1.0\n",
      "423      682  2024_02_12_09_51_21_00682.jpg  ...       840.6            2.0\n",
      "428      700  2024_02_12_09_51_21_00700.jpg  ...      1435.5            3.0\n",
      "431      705  2024_02_12_09_51_21_00705.jpg  ...      1599.7            4.0\n",
      "471      760  2024_02_12_09_51_21_00760.jpg  ...      3414.5            5.0\n",
      "481      787  2024_02_12_09_51_21_00787.jpg  ...      4305.7            6.0\n",
      "504      811  2024_02_12_09_51_21_00811.jpg  ...      5097.5            7.0\n",
      "517      826  2024_02_12_09_51_21_00826.jpg  ...      5593.3            8.0\n",
      "529      839  2024_02_12_09_51_21_00839.jpg  ...      6021.3            9.0\n",
      "533      844  2024_02_12_09_51_21_00844.jpg  ...      6186.8           10.0\n",
      "535      850  2024_02_12_09_51_21_00850.jpg  ...      6384.6           11.0\n",
      "550      867  2024_02_12_09_51_21_00867.jpg  ...      6945.7           12.0\n",
      "559      877  2024_02_12_09_51_21_00877.jpg  ...      7275.5           13.0\n",
      "615      935  2024_02_12_09_51_21_00935.jpg  ...      9189.4           14.0\n",
      "\n",
      "[14 rows x 33 columns]\n",
      "\n",
      "Group: 3\n",
      "     frameNr                          fName  ...  FixLatency  FixationOrder\n",
      "645      992  2024_02_12_09_51_21_00992.jpg  ...       539.3            1.0\n",
      "647     1002  2024_02_12_09_51_21_01002.jpg  ...       868.3            2.0\n",
      "651     1007  2024_02_12_09_51_21_01007.jpg  ...      1034.2            3.0\n",
      "659     1016  2024_02_12_09_51_21_01016.jpg  ...      1331.2            4.0\n",
      "683     1042  2024_02_12_09_51_21_01042.jpg  ...      2188.5            5.0\n",
      "709     1069  2024_02_12_09_51_21_01069.jpg  ...      3079.3            6.0\n",
      "717     1078  2024_02_12_09_51_21_01078.jpg  ...      3376.6            7.0\n",
      "749     1124  2024_02_12_09_51_21_01124.jpg  ...      4895.3            8.0\n",
      "770     1147  2024_02_12_09_51_21_01147.jpg  ...      5654.0            9.0\n",
      "801     1179  2024_02_12_09_51_21_01179.jpg  ...      6709.4           10.0\n",
      "807     1187  2024_02_12_09_51_21_01187.jpg  ...      6974.3           11.0\n",
      "836     1217  2024_02_12_09_51_21_01217.jpg  ...      7963.5           12.0\n",
      "\n",
      "[12 rows x 33 columns]\n",
      "\n",
      "Group: 4\n",
      "      frameNr                          fName  ...  FixLatency  FixationOrder\n",
      "886      1319  2024_02_12_09_51_21_01319.jpg  ...         0.0            1.0\n",
      "897      1332  2024_02_12_09_51_21_01332.jpg  ...       428.5            2.0\n",
      "906      1343  2024_02_12_09_51_21_01343.jpg  ...       791.6            3.0\n",
      "911      1351  2024_02_12_09_51_21_01351.jpg  ...      1055.3            4.0\n",
      "916      1358  2024_02_12_09_51_21_01358.jpg  ...      1286.5            5.0\n",
      "925      1369  2024_02_12_09_51_21_01369.jpg  ...      1650.1            6.0\n",
      "941      1386  2024_02_12_09_51_21_01386.jpg  ...      2210.4            7.0\n",
      "1035     1487  2024_02_12_09_51_21_01487.jpg  ...      5544.1            8.0\n",
      "1038     1503  2024_02_12_09_51_21_01503.jpg  ...      6072.5            9.0\n",
      "1068     1534  2024_02_12_09_51_21_01534.jpg  ...      7094.4           10.0\n",
      "1082     1550  2024_02_12_09_51_21_01550.jpg  ...      7623.1           11.0\n",
      "1099     1574  2024_02_12_09_51_21_01574.jpg  ...      8414.5           12.0\n",
      "\n",
      "[12 rows x 33 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\2560102956.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fl_df['FixLatency'] = fl_df.sampTime - fl_df.targSampTime\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\2560102956.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fl_df['FixLatency'] = fl_df['FixLatency'].clip(lower=0)\n"
     ]
    }
   ],
   "source": [
    "# Get fixation latency\n",
    "\n",
    "def getFixationLatency(df):    \n",
    "\n",
    "    # Get timestamp of when target was presented and add it to the dataframe\n",
    "\n",
    "    # 1) get the first time sample when the target is presented\n",
    "    sampTime_df = df.drop_duplicates(subset=['trialNr'],  keep='first', ignore_index=True)\n",
    "\n",
    "    # 2)extract the columns needed\n",
    "    sampTime_df = sampTime_df[['trialNr', 'sampTime']]\n",
    "\n",
    "    # 3) rename the columns so they would be added\n",
    "    sampTime_df.columns = ['trialNr', 'targSampTime']\n",
    "\n",
    "    # 4) merge the target time into the main df (one time per trial)\n",
    "    df = pd.merge(df, sampTime_df, on=\"trialNr\")\n",
    "\n",
    "    # Extract saccade latencies\n",
    "\n",
    "    # 1) select only rows where fixation started\n",
    "    fl_df = df[df.FixStartEnd == 'fix_start']\n",
    "\n",
    "    # 2) select only rows with large enough preceeding saccade\n",
    "#     fl_df = fl_df[fl_df.DistFromPrevFix > 300]\n",
    "\n",
    "    # 3) compute first fixation duration (saccade latency)\n",
    "    fl_df['FixLatency'] = fl_df.sampTime - fl_df.targSampTime\n",
    "\n",
    "    # 4) remove rows where negative Saccade Latencies for trials where no fixation end is present\n",
    "#     fl_df = fl_df[fl_df.FixLatency > 0]\n",
    "\n",
    "    # Clip the negative values to zero. This ensures that fixations that carry over and do not have fix_start have a zero latency\n",
    "    fl_df['FixLatency'] = fl_df['FixLatency'].clip(lower=0)\n",
    "\n",
    "    \n",
    "    # 5)\n",
    "    # Initialize an empty list to hold the groups\n",
    "    fixorder_groups = []\n",
    "    \n",
    "    for name, group in fl_df.groupby('trialNr'):\n",
    "        \n",
    "        # Add a new column with the order (rank) of the values\n",
    "        # 'method='first'' ensures that the order respects the original order in case of ties\n",
    "        group['FixationOrder'] = group['FixLatency'].rank()\n",
    "        \n",
    "        # Append the modified group to the list\n",
    "        fixorder_groups.append(group)\n",
    "        \n",
    "        print(f'Group: {name}')\n",
    "        print(group)\n",
    "        print()        \n",
    "    \n",
    "    # Concatenate all the modified groups back into a single DataFrame\n",
    "    fl_df_modified = pd.concat(fixorder_groups)\n",
    "    \n",
    "    # Extract the columns needed\n",
    "    fl_df_modified = fl_df_modified[['sampTime', 'FixLatency', 'FixationOrder']]\n",
    "    \n",
    "    # Filter out all rows except fix_start and fix_end\n",
    "    df_start_end = df[df['FixStartEnd'].isin(['fix_start', 'fix_end'])]\n",
    "\n",
    "    # Merge the variable into main df\n",
    "    df_modified = pd.merge(df_start_end, fl_df_modified, on=[\"sampTime\"], how=\"left\")\n",
    "    \n",
    "    # Get only fix_start events\n",
    "    df_fix_start = df_modified[df_modified.FixStartEnd == 'fix_start']\n",
    "    # Drop the FixDur column, which should be empty for fix_start events\n",
    "    df_fix_start = df_fix_start.drop('FixDur', axis=1)\n",
    "\n",
    "    \n",
    "    # Get only fix_end events\n",
    "    df_fix_end = df_modified[df_modified.FixStartEnd == 'fix_end']\n",
    "    # Seledct only the relevant events from fix_end events\n",
    "    df_fix_end = df_fix_end[[\"FixXPos\", \"FixYPos\", \"FixDur\"]]\n",
    "    \n",
    "    \n",
    "    # Merge fix start and end for the same fixations\n",
    "    df_final = pd.merge(df_fix_start, df_fix_end, on=[\"FixXPos\", \"FixYPos\"])\n",
    "    \n",
    "    return df_final, df_modified\n",
    "\n",
    "b, df_modified = getFixationLatency(df1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73379031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20\n",
      "15 15\n",
      "Trial 1 starts with fix_end\n",
      "Trial 1 ends with fix_start\n",
      "14 15\n",
      "Trial 2 starts with fix_end\n",
      "12 13\n",
      "Trial 3 starts with fix_end\n",
      "12 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixStartEnd.iloc[0] = 'fix_end_carryover_inserted_start'\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixDur.iloc[0] = group.sampTime.iloc[0] - group.targSampTime.iloc[0]\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixStartEnd.iloc[-1] = 'fix_start_carryover_inserted_end'\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixDur.iloc[-1] = 0\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixStartEnd.iloc[0] = 'fix_end_carryover_inserted_start'\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixDur.iloc[0] = group.sampTime.iloc[0] - group.targSampTime.iloc[0]\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixStartEnd.iloc[0] = 'fix_end_carryover_inserted_start'\n",
      "C:\\Users\\artem\\AppData\\Local\\Temp\\ipykernel_31220\\4285417200.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  group.FixDur.iloc[0] = group.sampTime.iloc[0] - group.targSampTime.iloc[0]\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "trial_duration = 10000 # maximum trial duration\n",
    "fixcarryover_groups = []\n",
    "\n",
    "for name, group in df_modified.groupby('trialNr'):\n",
    "    fix_start = group[group.FixStartEnd == 'fix_start'].FixStartEnd\n",
    "    fix_end = group[group.FixStartEnd == 'fix_end'].FixStartEnd\n",
    "    \n",
    "    print(fix_start.count(), fix_end.count())\n",
    "    \n",
    "    # if trial starts with fixation end, we need to add a fixation start event\n",
    "    if group.FixStartEnd.iloc[0] == 'fix_end':\n",
    "        print(f'Trial {name} starts with fix_end')\n",
    "        \n",
    "        # Add extra fixation start event\n",
    "        \n",
    "        # Reset variable in the first row\n",
    "        group.FixStartEnd.iloc[0] = 'fix_end_carryover_inserted_start'\n",
    "        group.FixDur.iloc[0] = group.sampTime.iloc[0] - group.targSampTime.iloc[0]\n",
    "        \n",
    "        # Insert a fix_start event\n",
    "        # Make a copy of the first row with a new index\n",
    "        first_row = group.iloc[0:1].copy()\n",
    "        first_row.index = [-1]  # Assign a negative index\n",
    "        first_row.FixStartEnd = 'fix_start_carryover_inserted_start'\n",
    "        first_row.FixDur = 0\n",
    "        first_row.FixLatency = 0\n",
    "        \n",
    "        \n",
    "        # Prepend the copied first row to the original DataFrame\n",
    "        group = pd.concat([first_row, group])\n",
    "\n",
    "        # Reset the index if you want a continuous numeric index\n",
    "        group = group.sort_index().reset_index(drop=True)\n",
    "        \n",
    "        # Now we need to re-rank the order of fixations in the trial, since we added a new one in the beginning\n",
    "        group['FixationOrder'] = group['FixLatency'].rank()\n",
    "        \n",
    "\n",
    "    # if trial ends with fixation start, we need to add a fixation end event\n",
    "    if group.FixStartEnd.iloc[-1] == 'fix_start':\n",
    "        print(f'Trial {name} ends with fix_start')\n",
    "        \n",
    "        # Add fixation end event\n",
    "        # Reset variable in the last row\n",
    "        group.FixStartEnd.iloc[-1] = 'fix_start_carryover_inserted_end'\n",
    "        group.FixDur.iloc[-1] = 0\n",
    "        \n",
    "        # Insert a fix_start event\n",
    "        # Make a copy of the last row with a ne index\n",
    "        last_row = group.iloc[[-1]].copy()        \n",
    "        last_row.index = last_row.index+1 # Assign the next index\n",
    "        last_row.FixStartEnd = 'fix_end_carryover_inserted_end'\n",
    "        last_row.FixDur = (last_row.targSampTime + trial_duration) - last_row.sampTime\n",
    "        last_row.FixLatency = 0\n",
    "        last_row.FixationOrder= 0\n",
    "        \n",
    "        # Append the copied first row to the original DataFrame\n",
    "        group = pd.concat([group, last_row])\n",
    "\n",
    "        # Reset the index if you want a continuous numeric index\n",
    "        group = group.sort_index().reset_index(drop=True)                      \n",
    "      \n",
    "        if c ==0:\n",
    "            d = group\n",
    "            c==0\n",
    "       \n",
    "    # Accumulate groups into a list\n",
    "    fixcarryover_groups.append(group)\n",
    "\n",
    "# Concatenate all the modified groups back into a single DataFrame\n",
    "fc_df = pd.concat(fixcarryover_groups)\n",
    "\n",
    "# Add a new columns labeling normal and carryover fixations\n",
    "# fc_df['FixCarryOver'] = np.where(fc_df['FixStartEnd'].isin(['fix_start', 'fix_end']), 'fix_within_trial', fc_df['FixStartEnd'])\n",
    "\n",
    "# Merge all fixation events\n",
    "# Get only fix_start events\n",
    "df_fix_start = fc_df[fc_df.FixStartEnd == 'fix_start']\n",
    "# Drop the FixDur column, which should be empty for fix_start events\n",
    "df_fix_start = df_fix_start.drop('FixDur', axis=1)\n",
    "\n",
    "# Get only fix_end events\n",
    "df_fix_end = fc_df[fc_df.FixStartEnd == 'fix_end']\n",
    "# Select only the relevant events from fix_end events\n",
    "df_fix_end = df_fix_end[[\"FixXPos\", \"FixYPos\", \"FixDur\"]]\n",
    "\n",
    "\n",
    "# Merge fix start and end for the same fixations\n",
    "# Merge df_fix_start and df_fix_end\n",
    "df_merged = pd.merge(df_fix_start, df_fix_end, on=[\"FixXPos\", \"FixYPos\"])\n",
    "\n",
    "\n",
    "# Get only fix_start_carryover_inserted_start events\n",
    "df_fix_start_insert_start = fc_df[fc_df.FixStartEnd == 'fix_start_carryover_inserted_start']\n",
    "df_fix_start_insert_start = df_fix_start_insert_start.drop('FixDur', axis=1)\n",
    "\n",
    "# Get only fix_end_carryover_inserted_start events\n",
    "df_fix_end_insert_start = fc_df[fc_df.FixStartEnd == 'fix_end_carryover_inserted_start']\n",
    "# Select only the relevant events from fix_end events\n",
    "df_fix_end_insert_start = df_fix_end_insert_start[[\"FixXPos\", \"FixYPos\", \"FixDur\"]]\n",
    "\n",
    "df_merged_insert_start = pd.merge(df_fix_start_insert_start, df_fix_end_insert_start, on=[\"FixXPos\", \"FixYPos\"])\n",
    "\n",
    "# Get only fix_start_carryover_inserted_end events\n",
    "df_fix_start_insert_end = fc_df[fc_df.FixStartEnd == 'fix_start_carryover_inserted_end']\n",
    "df_fix_start_insert_end = df_fix_start_insert_end.drop('FixDur', axis=1)\n",
    "\n",
    "# Get only fix_end_carryover_inserted_end events\n",
    "df_fix_end_insert_end = fc_df[fc_df.FixStartEnd == 'fix_end_carryover_inserted_end']\n",
    "# Select only the relevant events from fix_end events\n",
    "df_fix_end_insert_end = df_fix_end_insert_end[[\"FixXPos\", \"FixYPos\", \"FixDur\"]]\n",
    "\n",
    "df_merged_insert_end = pd.merge(df_fix_start_insert_end, df_fix_end_insert_end, on=[\"FixXPos\", \"FixYPos\"])\n",
    "\n",
    "\n",
    "# Now concatenate all carryover fixations, inserted_start and inserted_end\n",
    "df_carryover = pd.concat([df_merged_insert_start, df_merged_insert_end], ignore_index=True)\n",
    "\n",
    "# Sort the combined DataFrame based on frameNr\n",
    "df_carryover = df_carryover.sort_values(by='frameNr')\n",
    "\n",
    "# Now concatenate carryover fixations with within_trial fixations\n",
    "df_final2 = pd.concat([df_merged, df_carryover], ignore_index=True)\n",
    "# Sort based on frameNr\n",
    "df_final2 = df_final2.sort_values(by='frameNr')\n",
    "\n",
    "\n",
    "           \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9894b",
   "metadata": {},
   "source": [
    "# Get all saccade latencies in a trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b81dc0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artem\\AppData\\Local\\Temp\\ipykernel_27576\\2575846625.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sl_df['SaccLat'] = sl_df.PrevFixSampTime - sl_df.targSampTime\n"
     ]
    }
   ],
   "source": [
    "# Get fixation latency\n",
    "\n",
    "def getSaccLat(df):    \n",
    "\n",
    "    # Get timestamp of when target was presented and add it to the dataframe\n",
    "\n",
    "    # 1) get the first time sample when the target is presented\n",
    "    sampTime_df = df.drop_duplicates(subset=['trialNr'],  keep='first', ignore_index=True)\n",
    "\n",
    "    # 2)extract the columns needed\n",
    "    sampTime_df = sampTime_df[['trialNr', 'sampTime']]\n",
    "\n",
    "    # 3) rename the columns so they would be added\n",
    "    sampTime_df.columns = ['trialNr', 'targSampTime']\n",
    "\n",
    "    # 4) merge the target time into the main df (one time per trial)\n",
    "    df = pd.merge(df, sampTime_df, on=\"trialNr\")\n",
    "\n",
    "    # Extract saccade latencies\n",
    "\n",
    "    # 1) select only rows where fixation started\n",
    "    sl_df = df[df.FixStartEnd == 'fix_start']\n",
    "\n",
    "    # 2) select only rows with large enough preceeding saccade\n",
    "#     sl_df = sl_df[sl_df.DistFromPrevFix > 300]\n",
    "\n",
    "    # 3) compute first fixation duration (saccade latency)\n",
    "    sl_df['SaccLat'] = sl_df.PrevFixSampTime - sl_df.targSampTime\n",
    "\n",
    "    # 4) remove rows where negative Saccade Latencies for trials where no fixation end is present\n",
    "    sl_df = sl_df[sl_df.SaccLat > 0]\n",
    "    \n",
    "    return sl_df\n",
    "\n",
    "# 5) plot saccade latencies\n",
    "sl_df = getSaccLat(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57357f",
   "metadata": {},
   "source": [
    "# Analyze all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce8b9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:/Users/artem/Dropbox/DeepEye_Pilots/VBL_Exp1/pilot_data/vbl_dressPilot_feb12/approved\\analysis' already exists.\n",
      "\n",
      "\n",
      "\n",
      "Importing and processing: \"C:/Users/artem/Dropbox/DeepEye_Pilots/VBL_Exp1/pilot_data/vbl_dressPilot_feb12/approved\\data\\2024_02_12_09_50_57\\2024_02_12_09_50_57_record.csv\"\n",
      "\tSearching for valid interpolation windows\n",
      "\tReplace interpolation windows with Steffen interpolation\n",
      "\t2-Means clustering started for averaged signal\n",
      "\tDetermining fixations based on clustering weight mean for averaged signal and separate eyes + 2*std\n",
      "\n",
      "\n",
      "I2MC took 0.6530296802520752s to finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\anaconda3\\envs\\default\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/bag/bag4.jpg', './resized_stimuli/bag/bag3.jpg', './resized_stimuli/bag/bag6.jpg', './resized_stimuli/bag/bag1.jpg', './resized_stimuli/bag/bag5.jpg', './resized_stimuli/bag/bag2.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/hat/hat1.jpg', './resized_stimuli/hat/hat6.jpeg', './resized_stimuli/hat/hat3.jpg', './resized_stimuli/hat/hat5.jpeg', './resized_stimuli/hat/hat4.jpeg', './resized_stimuli/hat/hat2.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/sunglasses/sunglasses6.jpg', './resized_stimuli/sunglasses/sunglasses1.jpg', './resized_stimuli/sunglasses/sunglasses2.jpg', './resized_stimuli/sunglasses/sunglasses5.jpg', './resized_stimuli/sunglasses/sunglasses4.jpeg', './resized_stimuli/sunglasses/sunglasses3.jpeg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/shoe/shoe6.jpg', './resized_stimuli/shoe/shoe5.jpg', './resized_stimuli/shoe/shoe4.jpg', './resized_stimuli/shoe/shoe1.jpg', './resized_stimuli/shoe/shoe2.jpg', './resized_stimuli/shoe/shoe3.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/dresses/Dress_2/Model_S3_Dress_2.png', './resized_stimuli/dresses/Dress_5/Model_S2_Dress_5.png', './resized_stimuli/dresses/Dress_3/Model_S1_Dress_3.png', './resized_stimuli/dresses/Dress_6/Model_XL_Dress_6.png', './resized_stimuli/dresses/Dress_4/Model_M_Dress_4.png', './resized_stimuli/dresses/Dress_1/Model_L_Dress_1.png']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "\n",
      "\n",
      "\n",
      "Importing and processing: \"C:/Users/artem/Dropbox/DeepEye_Pilots/VBL_Exp1/pilot_data/vbl_dressPilot_feb12/approved\\data\\2024_02_12_09_51_21\\2024_02_12_09_51_21_record.csv\"\n",
      "\tSearching for valid interpolation windows\n",
      "\tReplace interpolation windows with Steffen interpolation\n",
      "\t2-Means clustering started for averaged signal\n",
      "\tDetermining fixations based on clustering weight mean for averaged signal and separate eyes + 2*std\n",
      "\n",
      "\n",
      "I2MC took 0.6549999713897705s to finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\anaconda3\\envs\\default\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/bag/bag3.jpg', './resized_stimuli/bag/bag4.jpg', './resized_stimuli/bag/bag2.jpg', './resized_stimuli/bag/bag5.jpg', './resized_stimuli/bag/bag1.jpg', './resized_stimuli/bag/bag6.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/hat/hat6.jpeg', './resized_stimuli/hat/hat3.jpg', './resized_stimuli/hat/hat5.jpeg', './resized_stimuli/hat/hat2.jpg', './resized_stimuli/hat/hat4.jpeg', './resized_stimuli/hat/hat1.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/sunglasses/sunglasses1.jpg', './resized_stimuli/sunglasses/sunglasses2.jpg', './resized_stimuli/sunglasses/sunglasses6.jpg', './resized_stimuli/sunglasses/sunglasses4.jpeg', './resized_stimuli/sunglasses/sunglasses3.jpeg', './resized_stimuli/sunglasses/sunglasses5.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/shoe/shoe2.jpg', './resized_stimuli/shoe/shoe4.jpg', './resized_stimuli/shoe/shoe6.jpg', './resized_stimuli/shoe/shoe1.jpg', './resized_stimuli/shoe/shoe5.jpg', './resized_stimuli/shoe/shoe3.jpg']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "coordinates: [[355, 95], [835, 95], [1315, 95], [355, 635], [835, 635], [1315, 635]]\n",
      "imageNames: ['./resized_stimuli/dresses/Dress_2/Model_S1_Dress_2.png', './resized_stimuli/dresses/Dress_1/Model_M_Dress_1.png', './resized_stimuli/dresses/Dress_3/Model_S3_Dress_3.png', './resized_stimuli/dresses/Dress_5/Model_L_Dress_5.png', './resized_stimuli/dresses/Dress_6/Model_XL_Dress_6.png', './resized_stimuli/dresses/Dress_4/Model_S2_Dress_4.png']\n",
      "\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n",
      "Plotting stimuli images\n"
     ]
    }
   ],
   "source": [
    "# Path to data folders\n",
    "# path = 'D:/Dropbox/DeepEye_Pilots/VBL_Exp1/pilot_data/vbl_dressPilot_feb12/approved'\n",
    "path = 'C:/Users/artem/Dropbox/DeepEye_Pilots/VBL_Exp1/pilot_data/vbl_dressPilot_feb12/approved'\n",
    "\n",
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory '{directory_path}' was created.\")\n",
    "    except FileExistsError:\n",
    "        # The directory already exists, no need to create it.\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n",
    "\n",
    "# definde data ana analysis directories and create them if they don't exist yet\n",
    "path_to_data = os.path.join(path, 'data')\n",
    "path_to_analysis = os.path.join(path, 'analysis')\n",
    "create_directory_if_not_exists(path_to_analysis)\n",
    "\n",
    "output_dfs = []\n",
    "# get all folder names\n",
    "folder_names = os.listdir(path_to_data)\n",
    "\n",
    "\n",
    "# read and process original datafile for each participant\n",
    "for fn in folder_names:\n",
    "    path_to_file = os.path.join(path_to_data, fn, fn+'_record.csv')       \n",
    "        \n",
    "    df = pd.read_csv(path_to_file)\n",
    "    df1 = extract_fixations(df, path_to_file)\n",
    "    plot2d(df1, fn, path_to_analysis, bboxes=True, stimuli=True)\n",
    "#     output_dfs.append(novelty_index(df1, fn))\n",
    "\n",
    "# output_df = pd.concat(output_dfs)\n",
    "# output_df.to_csv(os.path.join(path_to_analysis, 'allSubjects_NoveltyIndex.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f73ec",
   "metadata": {},
   "source": [
    "# Interactive 2D plot for every trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5290096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Importing and processing: \"C:/Users/artem/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/Test_PreferentialViewing/Pilot_PreferentialViewing/Young/Approved\\data\\2024_01_26_15_38_25\\2024_01_26_15_38_25_record.csv\"\n",
      "\tSearching for valid interpolation windows\n",
      "\tReplace interpolation windows with Steffen interpolation\n",
      "\t2-Means clustering started for averaged signal\n",
      "\tDetermining fixations based on clustering weight mean for averaged signal and separate eyes + 2*std\n",
      "\n",
      "\n",
      "I2MC took 3.3248491287231445s to finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artem\\anaconda3\\envs\\default\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "C:\\Users\\artem\\anaconda3\\envs\\default\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "# specify filename to plot interactively\n",
    "# Path to data folders\n",
    "path = 'C:/Users/artem/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/Test_PreferentialViewing/Pilot_PreferentialViewing/Young/Approved'\n",
    "# path_to_folders = 'D:/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/Test_MullerLyer/pilot'\n",
    "\n",
    "path_to_data = os.path.join(path, 'data')\n",
    "\n",
    "fn = '2024_01_26_15_38_25' #'2024_01_26_17_16_28'\n",
    "path_to_file = os.path.join(path_to_data, fn, fn+'_record.csv')       \n",
    "\n",
    "# get fixations\n",
    "df = pd.read_csv(path_to_file)\n",
    "df1 = extract_fixations(df, path_to_file)\n",
    "\n",
    "# Create a figure and initialize the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_updater = PlotUpdater(ax, df1, plot_type='2D')\n",
    "plot_updater.update_plot()\n",
    "\n",
    "# Position the 'Next' and 'Previous' buttons at the bottom of the plot\n",
    "button_ax_prev = plt.axes([0.6, 0.01, 0.1, 0.05])\n",
    "button_ax_next = plt.axes([0.8, 0.01, 0.1, 0.05])\n",
    "\n",
    "plot_updater.button_next.ax = button_ax_next\n",
    "plot_updater.button_prev.ax = button_ax_prev\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8436f5",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd6496e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/artem/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/Test_PreferentialViewing/Pilot_PreferentialViewing/Young/Approved'\n",
    "path_to_analysis = os.path.join(path, 'analysis')\n",
    "\n",
    "output_df = pd.read_csv(os.path.join(path_to_analysis, 'allSubjects_NoveltyIndex.csv'))\n",
    "\n",
    "# filter out excluded participants\n",
    "# '2024_01_15_14_19_20' was the second time, '2024_01_26_17_16_28' too few frames\n",
    "output_df = output_df[~output_df['deepeye-id'].isin(['2024_01_15_14_19_20', '2024_01_26_17_16_28'])]\n",
    "\n",
    "# group data\n",
    "total_count = output_df.groupby(['pp_id', 'deepeye-id']).noveltyIdx_fixCountProp.count()\n",
    "fixCountProp = output_df.groupby(['pp_id', 'deepeye-id']).noveltyIdx_fixCountProp.mean()\n",
    "fixDurProp = output_df.groupby(['pp_id', 'deepeye-id']).noveltyIdx_fixDurProp.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f60830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

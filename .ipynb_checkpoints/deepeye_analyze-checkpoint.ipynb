{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad9d858",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02c05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import astropy.convolution as krn\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece3bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHeat(screenRes, xPos, yPos):\n",
    "        xMax = screenRes[0]\n",
    "        yMax = screenRes[1]\n",
    "        xMin = 0\n",
    "        yMin = 0\n",
    "        kernelPar = 50\n",
    "\n",
    "        # Input handeling\n",
    "        xlim = np.logical_and(xPos < xMax, xPos > xMin)\n",
    "        ylim = np.logical_and(yPos < yMax, yPos > yMin)\n",
    "        xyLim = np.logical_and(xlim, ylim)\n",
    "        dataX = xPos[xyLim]\n",
    "        dataX = np.floor(dataX)\n",
    "        dataY = yPos[xyLim]\n",
    "        dataY = np.floor(dataY)\n",
    "\n",
    "        # initiate map and gauskernel\n",
    "        gazeMap = np.zeros([int((xMax-xMin)),int((yMax-yMin))])+0.0001\n",
    "        gausKernel = krn.Gaussian2DKernel(kernelPar)\n",
    "\n",
    "        # Rescale the position vectors (if xmin or ymin != 0)\n",
    "        dataX -= xMin\n",
    "        dataY -= yMin\n",
    "\n",
    "        # Now extract all the unique positions and number of samples\n",
    "        xy = np.vstack((dataX, dataY)).T\n",
    "        uniqueXY, idx, counts = uniqueRows(xy)\n",
    "        uniqueXY = uniqueXY.astype(int)\n",
    "        # populate the gazeMap\n",
    "        gazeMap[uniqueXY[:,0], uniqueXY[:,1]] = counts\n",
    "\n",
    "        # Convolve the gaze with the gauskernel\n",
    "        heatMap = np.transpose(krn.convolve_fft(gazeMap,gausKernel))\n",
    "        heatMap = heatMap/np.max(heatMap)\n",
    "\n",
    "        return heatMap\n",
    "\n",
    "def uniqueRows(x):\n",
    "    y = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, idx, counts = np.unique(y, return_index=True, return_counts = True)\n",
    "    uniques = x[idx]\n",
    "    return uniques, idx, counts\n",
    "\n",
    "\n",
    "def np_euclidean_distance(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sqrt(np.sum(np.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def dot_error(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    eucl_dist = np_euclidean_distance(y_true, y_pred)\n",
    "    # Get indices of unique dot positions\n",
    "    u, indices = np.unique(y_true, axis=0, return_inverse=True)\n",
    "    # Make dataframe for each sample of unique dot label and error distance\n",
    "    df_dict = {'unique_dot': indices, 'eucl_distance': eucl_dist, 'true_x': y_true[:,0],\n",
    "                'true_y': y_true[:,1], 'pred_x': y_pred[:,0], 'pred_y': y_pred[:,1]}\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    # Group by unique dot position, compute median error per dot, average across dots\n",
    "    mean_dot_error = df.groupby('unique_dot').eucl_distance.median().mean()\n",
    "    std_dot_error = df.groupby('unique_dot').eucl_distance.median().std()\n",
    "\n",
    "    return float(mean_dot_error), df, float(std_dot_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d127f7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n",
      "last: 9\n",
      "last: 13\n",
      "last: 13\n",
      "last: 9\n"
     ]
    }
   ],
   "source": [
    "path_to_folders = 'C:/Users/artem/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/online/complete'\n",
    "# path_to_folders = 'D:/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/online'\n",
    "\n",
    "# get all folder names\n",
    "folder_names = os.listdir(path_to_folders)\n",
    "\n",
    "pp_list = []\n",
    "for fn in folder_names:\n",
    "    path = os.path.join(path_to_folders, fn, fn+'_test_all.csv')       \n",
    "        \n",
    "    df = pd.read_csv(path)\n",
    "        \n",
    "    \n",
    "    # Find the headers via duplicates and use it to split into datasets\n",
    "    # Make indices of datasets\n",
    "    mask_dup = df.duplicated(keep=False)\n",
    "    idx_dup = df.index[mask_dup == True].tolist()\n",
    "    idx_dup[:0] = [-1] # add lower index\n",
    "    idx_dup.extend([df.shape[0]]) # add upper index\n",
    "    \n",
    "    # Use indices to parse datasets\n",
    "    df_list = []\n",
    "    count_datasets = 0\n",
    "    last_numCalibDots = []\n",
    "    for i in range(len(idx_dup)):\n",
    "        if i < len(idx_dup) - 1:\n",
    "            a = df.iloc[idx_dup[i]+1:idx_dup[i+1]]\n",
    "            a = a.apply(pd.to_numeric, errors='ignore') # when header is written twice, some floats are str, fix this \n",
    "            a['dataset_num'] = count_datasets\n",
    "            a['eucl_dist_px_orig'] = np_euclidean_distance(np.array(a[['x','y']]), np.array(a[['user_pred_px_x','user_pred_px_y']]))\n",
    "            scale_cm_in_px = a.scrW_cm/a.resX\n",
    "            a['eucl_dist_cm_orig'] = a.eucl_dist_px_orig * scale_cm_in_px \n",
    "            \n",
    "            if pd.api.types.is_string_dtype(a.sona_pp_id) == True:\n",
    "                a['platform'] = 'PROLIFIC'\n",
    "            else:\n",
    "                a['platform'] = 'SONA'\n",
    "           \n",
    "            # Label 25-dot conditions based on preceeding dataset\n",
    "            if a.numCalibDots.iloc[0] == 25:\n",
    "                print(f'last: {last_numCalibDots[-1]}')\n",
    "                if last_numCalibDots[-1] == 9:\n",
    "                    a['condition'] = '25_9'\n",
    "                elif last_numCalibDots[-1] == 13:\n",
    "                    a['condition'] = '25_13'\n",
    "                \n",
    "            else:\n",
    "                a['condition'] = a.numCalibDots.astype(str)\n",
    "                \n",
    "            last_numCalibDots.append(a.numCalibDots.iloc[-1]) # log last value                \n",
    "            \n",
    "            # Accumulate all dataset per subject\n",
    "            df_list.append(a)\n",
    "            count_datasets += 1\n",
    "    \n",
    "    \n",
    "    # if there are more than 4 datasets, remove the recalibrated ones, pick the last one\n",
    "    last_numCalibDots = pd.Series(last_numCalibDots)\n",
    "    idx_good_datasets = last_numCalibDots.loc[last_numCalibDots.shift(-1) != last_numCalibDots] # shift dataset by one row and get indices\n",
    "    df_list = [df_list[i] for i in list(idx_good_datasets.index)] # pick only the 4 datasets\n",
    "    assert(len(df_list) == 4)\n",
    "    \n",
    "    # Concatenate all datasets per subject\n",
    "    b = pd.concat(df_list)\n",
    "    \n",
    "    # Add a subj_nr column\n",
    "    b['subj_nr'] = fn    \n",
    "    \n",
    "    # Accumulate datasets across subjects\n",
    "    pp_list.append(b)\n",
    "\n",
    "# Concatenate all subjects in one df\n",
    "df_all = pd.concat(pp_list)\n",
    "\n",
    "\n",
    "\n",
    "# Every display resolution is scaled to this one since all dots are drawn in % display size in px\n",
    "target_resX = 1280.0\n",
    "target_resY = 800.0    \n",
    "\n",
    "# To do:\n",
    "# How many attempts\n",
    "# Filter out failed last attemtps\n",
    "# How to deal with missing data for some dots (e.g. for '2023_04_13_13_53_28')\n",
    "\n",
    "df_all = df_all.reset_index()\n",
    "\n",
    "\n",
    "# Select subset\n",
    "\"\"\"\n",
    "\n",
    "'2023_04_15_11_42_39' - amazing performance, but did not do 25_9\n",
    "\"\"\"\n",
    "# df_all = df_all[df_all.numCalibDots == 9]\n",
    "# df_all = df_all[(df_all.subj_nr == '2023_04_15_12_22_19')]\n",
    "# Exclude subjects\n",
    "df_all = df_all[df_all.subj_nr != '2023_04_07_13_59_57'] # my pilot data\n",
    "df_all = df_all[df_all.subj_nr != '2023_04_07_13_45_47'] # my pilot data\n",
    "\n",
    "\n",
    "# user_predictions_px = np.array(df_all[['user_pred_px_x', 'user_pred_px_y']])\n",
    "df_all['user_pred_px_x_scaled'] = df_all.user_pred_px_x/df_all.resX * target_resX\n",
    "df_all['user_pred_px_y_scaled'] = df_all.user_pred_px_y/df_all.resY * target_resY\n",
    "# ground_truths_px = np.array(df_all[['x','y']])\n",
    "df_all['x_scaled'] = np.round(df_all.x/df_all.resX * target_resX)\n",
    "df_all['y_scaled'] = np.round(df_all.y/df_all.resY * target_resY)\n",
    "\n",
    "df_all['scale_cm_in_px'] = df_all.scrW_cm.astype(float)/df_all.resX.astype(float)\n",
    "scale_cm_in_px = df_all.scale_cm_in_px.mean() # average scaling factor\n",
    "# scale_cm_in_px = df_all.scrW_cm.astype(float)[0]/df_all.resX.astype(float)[0]  \n",
    "\n",
    "# Get indices of unique dot positions (unique rows)\n",
    "u, indices = np.unique(np.array([df_all.x_scaled, df_all.y_scaled]).T, axis=0, return_inverse=True)\n",
    "df_all['unique_dot'] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting mean E.d. and SD per condition\n",
    "\"\"\"\n",
    "\n",
    "df_all = df_all[df_all.condition == '9']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4)\n",
    "fig.set_size_inches((8.5, 11), forward=False)\n",
    "\n",
    "\n",
    "# Loop thru each condition\n",
    "count_plots = 0\n",
    "summary_df = []\n",
    "for name, i in df_all.groupby('condition'):\n",
    "    \n",
    "    # Get median per each unique dot, separately per subject and condition\n",
    "#     summary_df_ed = i.groupby(['subj_nr', 'condition', 'unique_dot'])[['eucl_dist_px_orig', 'eucl_dist_cm_orig']].median().reset_index()\n",
    "#     summary_df_std = i.groupby(['subj_nr', 'condition', 'unique_dot'])[['eucl_dist_px_orig', 'eucl_dist_cm_orig']].std().reset_index()\n",
    "    \n",
    "    # Loop thru each subject and unique dot\n",
    "    for _, j in i.groupby(['subj_nr', 'unique_dot']):\n",
    "        \n",
    "        # Get median gaze for each unique dot in pixels\n",
    "        j['median_pred_x'] = j.user_pred_px_x_scaled.median()\n",
    "        j['median_pred_y'] = j.user_pred_px_y_scaled.median()\n",
    "        \n",
    "        # Get euclidean distance from each gaze sample to median gaze for each dot\n",
    "        j['eucl_dist_gaze_to_median_px'] = np_euclidean_distance(np.array([j.user_pred_px_x_scaled, j.user_pred_px_y_scaled]).T, \n",
    "                              np.array([j.median_pred_x, j.median_pred_y]).T)\n",
    "        # Convert to cm\n",
    "        j['eucl_dist_gaze_to_median_cm'] = j.eucl_dist_gaze_to_median_px * scale_cm_in_px\n",
    "        \n",
    "        summary_df.append(j)\n",
    "        \n",
    "      \n",
    "    summary_df = pd.concat(summary_df)\n",
    "    # Get mean distance per subject (equivalent to standard deviation or 1/precision)\n",
    "    agg = summary_df.groupby(['subj_nr'])[['eucl_dist_gaze_to_median_cm']].mean().reset_index()\n",
    "      \n",
    "        \n",
    "#     # Aggregate over dots (mean E.d. and SD E.d.)\n",
    "#     agg_Ed = summary_df_ed.groupby(['subj_nr', 'condition'])[['eucl_dist_px_orig', 'eucl_dist_cm_orig']].mean().reset_index()\n",
    "#     agg_SD = summary_df_std.groupby(['subj_nr', 'condition'])[['eucl_dist_px_orig', 'eucl_dist_cm_orig']].mean().reset_index()\n",
    "    \n",
    "#     # Get subj nr for the largest error\n",
    "#     larg_err_subj = agg_Ed.where(agg_Ed.eucl_dist_cm_orig==agg_Ed.eucl_dist_cm_orig.max()).dropna().subj_nr\n",
    "#     print(f'Maximum E.d. error is: {agg_Ed.eucl_dist_cm_orig.max(), larg_err_subj}')\n",
    "#     print('\\nMean Euclidean distance:')\n",
    "#     print(agg_Ed)\n",
    "#     print('\\nStandard deviation of Euclidean distances:')\n",
    "#     print(agg_SD)\n",
    "    \n",
    "#     # Plot euclidean distances per subject\n",
    "#     ax[0, count_plots].title.set_text(f'Condition:{i.condition.iloc[0]}\\nEuclidean distances')\n",
    "#     ax[0, count_plots].set_ylim(0,4.5)\n",
    "#     ax[0, count_plots].scatter(np.ones(agg_Ed.eucl_dist_cm_orig.size),agg_Ed.eucl_dist_cm_orig)\n",
    "#     ax[0, count_plots].scatter(1,agg_Ed.eucl_dist_cm_orig.mean())\n",
    "    \n",
    "#     # Plot SD per subject\n",
    "#     ax[1, count_plots].title.set_text(f'Condition:{i.condition.iloc[0]}\\nSDs')\n",
    "#     ax[1, count_plots].set_ylim(0,4.5)\n",
    "#     ax[1, count_plots].scatter(np.ones(agg_SD.eucl_dist_cm_orig.size),agg_SD.eucl_dist_cm_orig)\n",
    "#     ax[1, count_plots].scatter(1,agg_SD.eucl_dist_cm_orig.mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    count_plots += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2344d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

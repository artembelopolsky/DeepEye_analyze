{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9d6fde",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f62cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import astropy.convolution as krn\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c91c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHeat(screenRes, xPos, yPos):\n",
    "        xMax = screenRes[0]\n",
    "        yMax = screenRes[1]\n",
    "        xMin = 0\n",
    "        yMin = 0\n",
    "        kernelPar = 50\n",
    "\n",
    "        # Input handeling\n",
    "        xlim = np.logical_and(xPos < xMax, xPos > xMin)\n",
    "        ylim = np.logical_and(yPos < yMax, yPos > yMin)\n",
    "        xyLim = np.logical_and(xlim, ylim)\n",
    "        dataX = xPos[xyLim]\n",
    "        dataX = np.floor(dataX)\n",
    "        dataY = yPos[xyLim]\n",
    "        dataY = np.floor(dataY)\n",
    "\n",
    "        # initiate map and gauskernel\n",
    "        gazeMap = np.zeros([int((xMax-xMin)),int((yMax-yMin))])+0.0001\n",
    "        gausKernel = krn.Gaussian2DKernel(kernelPar)\n",
    "\n",
    "        # Rescale the position vectors (if xmin or ymin != 0)\n",
    "        dataX -= xMin\n",
    "        dataY -= yMin\n",
    "\n",
    "        # Now extract all the unique positions and number of samples\n",
    "        xy = np.vstack((dataX, dataY)).T\n",
    "        uniqueXY, idx, counts = uniqueRows(xy)\n",
    "        uniqueXY = uniqueXY.astype(int)\n",
    "        # populate the gazeMap\n",
    "        gazeMap[uniqueXY[:,0], uniqueXY[:,1]] = counts\n",
    "\n",
    "        # Convolve the gaze with the gauskernel\n",
    "        heatMap = np.transpose(krn.convolve_fft(gazeMap,gausKernel))\n",
    "        heatMap = heatMap/np.max(heatMap)\n",
    "\n",
    "        return heatMap\n",
    "\n",
    "def uniqueRows(x):\n",
    "    y = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, idx, counts = np.unique(y, return_index=True, return_counts = True)\n",
    "    uniques = x[idx]\n",
    "    return uniques, idx, counts\n",
    "\n",
    "\n",
    "def np_euclidean_distance(y_true, y_pred):\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sqrt(np.sum(np.square(y_pred - y_true), axis=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291816c6",
   "metadata": {},
   "source": [
    "### Preprocess files and make dataframe\n",
    "1. Extract datasets based on the header row\n",
    "2. Label 25-dot dataset based on 9-dot calibration as 25_9, and on 13-dot calibration as 25_13\n",
    "3. Participants had 3 attempts to get calibration <3 cm. Keep only the successful calibration dataset (total=4)\n",
    "4. Scale pixels to standard dimensions. Calibration dots were presented as % display size in px\n",
    "5. Convert pixels to cm\n",
    "6. Assign each dot a unique label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9016ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folders\n",
    "# path_to_folders = 'C:/Users/artem/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/online/complete'\n",
    "path_to_folders = 'D:/Dropbox/Appliedwork/CognitiveSolutions/Projects/DeepEye/TechnicalReports/TechnicalReport1/online/complete'\n",
    "\n",
    "# get all folder names\n",
    "folder_names = os.listdir(path_to_folders)\n",
    "\n",
    "pp_list = []\n",
    "num_calib_attemtps = []\n",
    "for fn in folder_names:\n",
    "    path = os.path.join(path_to_folders, fn, fn+'_test_all.csv')       \n",
    "        \n",
    "    df = pd.read_csv(path)        \n",
    "    \n",
    "    # Find the headers via duplicates and use it to split into datasets   \n",
    "    mask_dup = df.duplicated(keep=False)\n",
    "    # Make indices of datasets\n",
    "    idx_dup = df.index[mask_dup == True].tolist()\n",
    "    idx_dup[:0] = [-1] # add lower index\n",
    "    idx_dup.extend([df.shape[0]]) # add upper index\n",
    "    \n",
    "    # Use indices to parse datasets\n",
    "    df_list = []\n",
    "    count_datasets = 0\n",
    "    last_numCalibDots = []\n",
    "    \n",
    "    for i in range(len(idx_dup)):\n",
    "        if i < len(idx_dup) - 1:\n",
    "            a = df.iloc[idx_dup[i]+1:idx_dup[i+1]]\n",
    "            a = a.apply(pd.to_numeric, errors='ignore') # when header is written twice, some floats are str, fix this \n",
    "            a['dataset_num'] = count_datasets\n",
    "            a['eucl_dist_px_orig'] = np_euclidean_distance(np.array(a[['x','y']]), np.array(a[['user_pred_px_x','user_pred_px_y']]))\n",
    "            scale_cm_in_px = a.scrW_cm/a.resX\n",
    "            a['eucl_dist_cm_orig'] = a.eucl_dist_px_orig * scale_cm_in_px \n",
    "            \n",
    "            if pd.api.types.is_string_dtype(a.sona_pp_id) == True:\n",
    "                a['platform'] = 'PROLIFIC'\n",
    "            else:\n",
    "                a['platform'] = 'SONA'\n",
    "           \n",
    "            # Label 25-dot conditions based on preceeding dataset\n",
    "            if a.numCalibDots.iloc[0] == 25:\n",
    "                # print(f'last: {last_numCalibDots[-1]}')\n",
    "                if last_numCalibDots[-1] == 9:\n",
    "                    a['condition'] = '25_9'\n",
    "                elif last_numCalibDots[-1] == 13:\n",
    "                    a['condition'] = '25_13'                \n",
    "            else:\n",
    "                a['condition'] = a.numCalibDots.astype(str)\n",
    "                \n",
    "            last_numCalibDots.append(a.numCalibDots.iloc[-1]) # log last value                \n",
    "            \n",
    "            # Accumulate all dataset per subject\n",
    "            df_list.append(a)\n",
    "            count_datasets += 1\n",
    "    \n",
    "    \n",
    "    # if there are more than 4 datasets, remove the recalibrated ones, pick the last one\n",
    "    last_numCalibDots = pd.Series(last_numCalibDots)\n",
    "    idx_good_datasets = last_numCalibDots.loc[last_numCalibDots.shift(-1) != last_numCalibDots] # shift dataset by one row and get indices\n",
    "    df_list = [df_list[i] for i in list(idx_good_datasets.index)] # pick only the 4 datasets\n",
    "    assert(len(df_list) == 4)\n",
    "    \n",
    "    # Concatenate all datasets per subject\n",
    "    b = pd.concat(df_list)\n",
    "    \n",
    "    # Add a subj_nr column\n",
    "    b['subj_nr'] = fn    \n",
    "    \n",
    "    # Count the number of calibration attempts per participant in a new df\n",
    "    unique, counts = np.unique(last_numCalibDots, return_counts=True)\n",
    "    c = pd.DataFrame(np.asarray((unique, counts)).T, columns=['num_calib_dots', 'frequency'])\n",
    "    c['subj_nr'] = fn\n",
    "    num_calib_attemtps.append(c)\n",
    "    num_calib_attempts_df = pd.concat(num_calib_attemtps).reset_index(drop=True)\n",
    "    \n",
    "        \n",
    "    # Accumulate datasets across subjects\n",
    "    pp_list.append(b)\n",
    "\n",
    "# Concatenate all subjects in one df\n",
    "df_all = pd.concat(pp_list)\n",
    "df_all = df_all.reset_index()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "'2023_04_15_11_42_39' - amazing performance, but did not do 25_9\n",
    "\"\"\"\n",
    "# Exclude subjects\n",
    "df_all = df_all[df_all.subj_nr != '2023_04_07_13_59_57'] # my pilot data\n",
    "df_all = df_all[df_all.subj_nr != '2023_04_07_13_45_47'] # my pilot data\n",
    "\n",
    "# Convert coordinates to standard resolution (target_resX and target_resY)\n",
    "# Every display resolution is scaled to this one since all dots are drawn in % display size in px\n",
    "target_resX = 1280.0\n",
    "target_resY = 800.0\n",
    "\n",
    "df_all['user_pred_px_x_scaled'] = df_all.user_pred_px_x/df_all.resX * target_resX\n",
    "df_all['user_pred_px_y_scaled'] = df_all.user_pred_px_y/df_all.resY * target_resY\n",
    "\n",
    "df_all['x_scaled'] = np.round(df_all.x/df_all.resX * target_resX)\n",
    "df_all['y_scaled'] = np.round(df_all.y/df_all.resY * target_resY)\n",
    "\n",
    "# Get an average scale for converting to px to cm\n",
    "df_all['scale_cm_in_px'] = df_all.scrW_cm.astype(float)/df_all.resX.astype(float)\n",
    "scale_cm_in_px = df_all.scale_cm_in_px.mean() # average scaling factor\n",
    "\n",
    "# Get indices of unique dot positions (unique rows)\n",
    "u, indices = np.unique(np.array([df_all.x_scaled, df_all.y_scaled]).T, axis=0, return_inverse=True)\n",
    "df_all['unique_dot'] = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e17196",
   "metadata": {},
   "source": [
    "### Plot heatmaps per each condition\n",
    "Gaze positions for all subjects are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc1f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(nrows=2, ncols=2)\n",
    "fig2.set_size_inches((8.5, 7.0), forward=False)\n",
    "fig2.tight_layout()\n",
    "\n",
    "subplot_cell = [[0,0],[0,1],[1,0], [1,1]] \n",
    "\n",
    "count_plots2 = 0\n",
    "\n",
    "# Iterate per condition\n",
    "for name, df in df_all.groupby('condition'):\n",
    "\n",
    "    # Make heatmap of all gaze points of all subjects\n",
    "    heatmap = makeHeat([target_resX, target_resY], np.array(df.user_pred_px_x_scaled), np.array(df.user_pred_px_y_scaled))    \n",
    "    \n",
    "    # Get median gaze prediction per dot\n",
    "    median_pred_x = df.groupby('unique_dot').user_pred_px_x_scaled.median()\n",
    "    median_pred_y = df.groupby('unique_dot').user_pred_px_y_scaled.median()  \n",
    "             \n",
    "    # Get ground truth per each dot\n",
    "    true_x = df.groupby('unique_dot').x_scaled.mean()\n",
    "    true_y = df.groupby('unique_dot').y_scaled.mean()   \n",
    "        \n",
    "    # calculate the distance between median gaze prediction to ground truth per each dot\n",
    "    offset = np_euclidean_distance(np.array([median_pred_x, median_pred_y]).T, np.array([true_x, true_y]).T)    \n",
    "   \n",
    "    # Convert to cm\n",
    "    offset_cm = offset *  scale_cm_in_px\n",
    "    \n",
    "    # Compute SD   \n",
    "    median_pred = df.groupby('unique_dot')[['user_pred_px_x_scaled','user_pred_px_y_scaled']].median()\n",
    "    median_pred.columns = ['median_pred_x', 'median_pred_y'] \n",
    "    # Add median prediction per dot into the original df\n",
    "    df = pd.merge(df, median_pred, on=\"unique_dot\", how=\"left\")\n",
    "    \n",
    "    # Compute euclidean distance from each gaze sample to median gaze for each dot  \n",
    "    df['eucl_dist_gaze_to_median_px'] = np_euclidean_distance(np.array([df.user_pred_px_x_scaled, df.user_pred_px_y_scaled]).T, \n",
    "                                                             np.array([df.median_pred_x, df.median_pred_y]).T)\n",
    "    \n",
    "    # Average e.d. for each dot\n",
    "    SD = df.groupby('unique_dot').eucl_dist_gaze_to_median_px.mean()\n",
    "    SD_cm = SD * scale_cm_in_px\n",
    "    \n",
    "    # Get subplot coordinates\n",
    "    row = subplot_cell[count_plots2][0]\n",
    "    column = subplot_cell[count_plots2][1]\n",
    "    \n",
    "    # Plot heatmap for current condition\n",
    "    ax2[row, column].imshow(heatmap, cmap=cm.hot, extent=[0, target_resX, target_resY, 0], alpha = 0.5, aspect='equal')  \n",
    "    # Plot true pos and predicted median errors, lines\n",
    "    ax2[row, column].scatter(df.x_scaled, df.y_scaled, c='g', s=40, alpha=0.5)\n",
    "    ax2[row, column].scatter(median_pred_x, median_pred_y, c='b', s=40)\n",
    "    ax2[row, column].plot([median_pred_x, true_x], [median_pred_y, true_y], c='black')\n",
    "    # Title\n",
    "    ax2[row, column].set_title(f'Condition:{df.condition.iloc[0]}\\n Offset: {np.round(offset_cm.mean(),1)}cm, SD:{np.round(SD_cm.mean(),1)}cm')\n",
    "    \n",
    "    # Plot offset values for each dot\n",
    "    for x,y,e in zip(np.array(true_x), np.array(true_y), np.round(offset_cm, 1)):\n",
    "        ax2[row, column].text(x, y, e, fontsize=10)\n",
    "    \n",
    "    count_plots2 += 1      \n",
    "  \n",
    "    \n",
    " # Save plot\n",
    "fig2.suptitle(f'N={df.subj_nr.unique().size}', fontsize=16)\n",
    "fig2.subplots_adjust(top=0.9)\n",
    "fig2.savefig('calibration.jpg', dpi=1000, pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e5dc2",
   "metadata": {},
   "source": [
    "### Calculate and plot offsets and SD per condition per subject\n",
    "\n",
    "1. Offset - euclidean distance from the median gaze prediction per each dot to the ground truth\n",
    "2. SD - mean of euclidean distances from each gaze prediction to the median gaze prediction for each dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c37b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=4)\n",
    "fig.set_size_inches((8.5, 11), forward=False)\n",
    "\n",
    "\n",
    "# Loop thru each condition\n",
    "count_plots = 0\n",
    "summary_df_all = []\n",
    "\n",
    "for name, i in df_all.groupby('condition'):\n",
    "    \n",
    "    # df for all subjects for each condition\n",
    "    summary_df = []    \n",
    "   \n",
    "    # Loop thru each subject and unique dot\n",
    "    for _, j in i.groupby(['subj_nr', 'unique_dot']):\n",
    "        \n",
    "        # Get median gaze for each unique dot in pixels\n",
    "        j['median_pred_x'] = j.user_pred_px_x_scaled.median()\n",
    "        j['median_pred_y'] = j.user_pred_px_y_scaled.median()\n",
    "        \n",
    "        # Get euclidean distance from each gaze sample to median gaze for each dot\n",
    "        j['eucl_dist_gaze_to_median_px'] = np_euclidean_distance(np.array([j.user_pred_px_x_scaled, j.user_pred_px_y_scaled]).T, \n",
    "                              np.array([j.median_pred_x, j.median_pred_y]).T)        \n",
    "        j['eucl_dist_gaze_to_median_cm'] = j.eucl_dist_gaze_to_median_px * scale_cm_in_px\n",
    "        \n",
    "        # Get euclidean distance from median gaze to ground truth (accuracy)\n",
    "        j['offset_px'] = np_euclidean_distance(np.array([j.median_pred_x, j.median_pred_y]).T, np.array([j.x_scaled, j.y_scaled]).T)  \n",
    "        j['offset_cm'] = j.offset_px * scale_cm_in_px\n",
    "        \n",
    "        summary_df.append(j)        \n",
    "    \n",
    "    # Concatenate datasets from all subjects  \n",
    "    summary_df = pd.concat(summary_df)\n",
    "    \n",
    "    # Get STD (mean distance of gaze_to_median per subject)\n",
    "    agg_SD = summary_df.groupby(['subj_nr'])[['eucl_dist_gaze_to_median_cm']].mean().reset_index()\n",
    "    \n",
    "    # Get offset\n",
    "    agg_OFFSET = summary_df.groupby(['subj_nr'])[['offset_cm']].mean().reset_index()\n",
    "    \n",
    "    # Plot offset per subject \n",
    "    ax[0, count_plots].title.set_text(f'Condition:{i.condition.iloc[0]}')\n",
    "    ax[0, count_plots].set_ylabel('Offset (cm)')\n",
    "    ax[0, count_plots].set_ylim(0,4.5)\n",
    "    ax[0, count_plots].scatter(np.ones(agg_OFFSET.offset_cm.size),agg_OFFSET.offset_cm)\n",
    "    ax[0, count_plots].scatter(1,agg_OFFSET.offset_cm.mean())\n",
    "    \n",
    "    # Plot SD per subject\n",
    "    ax[1, count_plots].title.set_text(f'Condition:{i.condition.iloc[0]}')\n",
    "    ax[1, count_plots].set_ylabel('SD (cm)')\n",
    "    ax[1, count_plots].set_ylim(0,4.5)\n",
    "    ax[1, count_plots].scatter(np.ones(agg_SD.eucl_dist_gaze_to_median_cm.size),agg_SD.eucl_dist_gaze_to_median_cm)\n",
    "    ax[1, count_plots].scatter(1,agg_SD.eucl_dist_gaze_to_median_cm.mean())        \n",
    "    \n",
    "    count_plots += 1\n",
    "    \n",
    "    # accumulate all dfs across conditions\n",
    "    summary_df_all.append(summary_df)\n",
    "\n",
    "# Concatenate datasets across conditions\n",
    "summary_df_all = pd.concat(summary_df_all)\n",
    "    \n",
    "# Save plot\n",
    "fig.tight_layout()\n",
    "fig.suptitle(f'N={df_all.subj_nr.unique().size}', fontsize=16)\n",
    "fig.subplots_adjust(top=0.9)\n",
    "fig.savefig('summary.jpg', dpi=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6617b8db",
   "metadata": {},
   "source": [
    "### T-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed3aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Offset summary ***\n",
      "condition\n",
      "13       1.548663\n",
      "25_13    1.823012\n",
      "25_9     1.775879\n",
      "9        1.183770\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "T-Test: 13 vs. 9\n",
      "TtestResult(statistic=2.9180903363958417, pvalue=0.0067389600215769264, df=29)\n",
      "T-Test: 25_13 vs. 25_9:\n",
      "TtestResult(statistic=0.26004137266347643, pvalue=0.7966692801734082, df=29)\n",
      "\n",
      "\n",
      "*** SD summary ***\n",
      "condition\n",
      "13       0.821722\n",
      "25_13    0.767932\n",
      "25_9     0.797018\n",
      "9        0.723226\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "T-Test: 13 vs. 9\n",
      "TtestResult(statistic=1.9516428886842374, pvalue=0.060698437984373396, df=29)\n",
      "T-Test: 25_13 vs. 25_9:\n",
      "TtestResult(statistic=-1.0117442410951574, pvalue=0.3200278588972269, df=29)\n",
      "\n",
      "\n",
      "*** For whom is calibration offset greater than 3.0cm? ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>condition</th>\n",
       "      <th>subj_nr</th>\n",
       "      <th>13</th>\n",
       "      <th>25_13</th>\n",
       "      <th>25_9</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "condition subj_nr  13  25_13  25_9   9\n",
       "0             NaN NaN    NaN   NaN NaN\n",
       "1             NaN NaN    NaN   NaN NaN\n",
       "2             NaN NaN    NaN   NaN NaN\n",
       "3             NaN NaN    NaN   NaN NaN\n",
       "4             NaN NaN    NaN   NaN NaN\n",
       "5             NaN NaN    NaN   NaN NaN\n",
       "6             NaN NaN    NaN   NaN NaN\n",
       "7             NaN NaN    NaN   NaN NaN\n",
       "8             NaN NaN    NaN   NaN NaN\n",
       "9             NaN NaN    NaN   NaN NaN\n",
       "10            NaN NaN    NaN   NaN NaN\n",
       "11            NaN NaN    NaN   NaN NaN\n",
       "12            NaN NaN    NaN   NaN NaN\n",
       "13            NaN NaN    NaN   NaN NaN\n",
       "14            NaN NaN    NaN   NaN NaN\n",
       "15            NaN NaN    NaN   NaN NaN\n",
       "16            NaN NaN    NaN   NaN NaN\n",
       "17            NaN NaN    NaN   NaN NaN\n",
       "18            NaN NaN    NaN   NaN NaN\n",
       "19            NaN NaN    NaN   NaN NaN\n",
       "20            NaN NaN    NaN   NaN NaN\n",
       "21            NaN NaN    NaN   NaN NaN\n",
       "22            NaN NaN    NaN   NaN NaN\n",
       "23            NaN NaN    NaN   NaN NaN\n",
       "24            NaN NaN    NaN   NaN NaN\n",
       "25            NaN NaN    NaN   NaN NaN\n",
       "26            NaN NaN    NaN   NaN NaN\n",
       "27            NaN NaN    NaN   NaN NaN\n",
       "28            NaN NaN    NaN   NaN NaN\n",
       "29            NaN NaN    NaN   NaN NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-tests offset\n",
    "offset = summary_df_all.groupby(['subj_nr','condition']).offset_cm.mean()\n",
    "offset = offset.unstack()\n",
    "\n",
    "print('*** Offset summary ***')\n",
    "print(offset.mean())\n",
    "print('\\n')\n",
    "print('T-Test: 13 vs. 9')\n",
    "print(stats.ttest_rel(np.array(offset['13']), np.array(offset['9'])))\n",
    "print('T-Test: 25_13 vs. 25_9:')\n",
    "print(stats.ttest_rel(np.array(offset['25_13']), np.array(offset['25_9'])))\n",
    "\n",
    "# T-tests SD\n",
    "SD = summary_df_all.groupby(['subj_nr','condition']).eucl_dist_gaze_to_median_cm.mean()\n",
    "SD = SD.unstack()\n",
    "print('\\n')\n",
    "print('*** SD summary ***')\n",
    "print(SD.mean())\n",
    "print('\\n')\n",
    "print('T-Test: 13 vs. 9')\n",
    "print(stats.ttest_rel(np.array(SD['13']), np.array(SD['9'])))\n",
    "print('T-Test: 25_13 vs. 25_9:')\n",
    "print(stats.ttest_rel(np.array(SD['25_13']), np.array(SD['25_9'])))    \n",
    "\n",
    "offset_cutoff = 3.0\n",
    "print('\\n')\n",
    "print(f'*** For whom is calibration offset greater than {offset_cutoff}cm? ***')\n",
    "x = offset.reset_index()\n",
    "x.where((x['13'] > offset_cutoff) | (x['9'] > offset_cutoff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce777ad5",
   "metadata": {},
   "source": [
    "### Overview of participants and the number of included gaze samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c5b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats:\n",
      "     platform              subj_nr                sona_pp_id  unique_dot\n",
      "0   PROLIFIC  2023_04_12_21_03_47  5bd022a50f10750001d898b0        1539\n",
      "1   PROLIFIC  2023_04_12_21_07_32  5eb179d6d6fcb726f9275f97        1669\n",
      "2   PROLIFIC  2023_04_12_23_30_03  5f29ed73209c1b1a178602e3        1690\n",
      "3   PROLIFIC  2023_04_13_07_33_30  62fbe4c86d484357b6adbc36        1578\n",
      "4   PROLIFIC  2023_04_13_08_53_13  62fb79e7c968f4fc555d1e2f        1338\n",
      "5   PROLIFIC  2023_04_13_13_32_18  57d5ab3a722df500017f3622        2092\n",
      "6   PROLIFIC  2023_04_13_13_53_28  5a67676a35f26b000149689d         678\n",
      "7   PROLIFIC  2023_04_13_14_57_48  5e89ec793c01b3685a00bb12        2210\n",
      "8   PROLIFIC  2023_04_13_17_54_55  640103805957bbf6a1326ed0        2373\n",
      "9   PROLIFIC  2023_04_13_18_25_35  5734b743d0f8fe10e4e4415d        1030\n",
      "10  PROLIFIC  2023_04_13_19_29_32  5c93b062c9d93b0015fcbc02        1759\n",
      "11  PROLIFIC  2023_04_13_20_23_24  5bb756696322c5000159756c        2203\n",
      "12  PROLIFIC  2023_04_13_20_46_16  5ef7c6094ffead5091899f98        1108\n",
      "13  PROLIFIC  2023_04_13_21_04_58  5d7ebf9e93902b0001965912        2224\n",
      "14  PROLIFIC  2023_04_13_21_32_06  60fe8de5380ae67bf7370ac8        2121\n",
      "15  PROLIFIC  2023_04_14_11_18_47  6272f5f3d37c1c9e88c115a2        2298\n",
      "16  PROLIFIC  2023_04_14_12_03_32  5d38419810801e001ae58771        1721\n",
      "17  PROLIFIC  2023_04_14_13_51_14  5e13a93a8640209e59703481        2269\n",
      "18  PROLIFIC  2023_04_14_14_52_27  634fde191ed73d0409412077        2137\n",
      "19  PROLIFIC  2023_04_14_17_12_17  5b2792b87f78160001463f90        1169\n",
      "20  PROLIFIC  2023_04_14_21_04_57  5e89ba09ec9b3364b1fc0bd1        1502\n",
      "21  PROLIFIC  2023_04_15_11_58_10  63eac36a81a068cd4b39e6bd        2310\n",
      "22  PROLIFIC  2023_04_15_12_22_19  63e51c3810f64b87f0e2dde5        1882\n",
      "23  PROLIFIC  2023_04_15_12_50_43  5c027a6dc776820001bd652f        1540\n",
      "24  PROLIFIC  2023_04_15_13_46_39  604e7009ec21c0e9b6990c6f        1400\n",
      "25      SONA  2023_04_12_11_19_28                     23333        1181\n",
      "26      SONA  2023_04_12_12_14_51                     23986        2201\n",
      "27      SONA  2023_04_12_13_00_41                     23584        1759\n",
      "28      SONA  2023_04_12_13_20_33                     23419        2094\n",
      "29      SONA  2023_04_13_09_27_35                     24008        2261\n"
     ]
    }
   ],
   "source": [
    "# Participant descriptive statistics (gaze samples per participant)\n",
    "descr_stats = summary_df_all.groupby(['platform', 'subj_nr', 'sona_pp_id'])['unique_dot'].count().reset_index()\n",
    "print(f'Descriptive Stats:\\n {descr_stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01817ea0",
   "metadata": {},
   "source": [
    "### Average number of unique dots per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f39f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean unique dots:\n",
      " condition\n",
      "13       12.766667\n",
      "25_13    24.400000\n",
      "25_9     23.933333\n",
      "9         8.700000\n",
      "Name: unique_dot, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Participant descriptive statistics (gaze samples per participant)\n",
    "descr_stats = summary_df_all.groupby(['condition','subj_nr','unique_dot'])['offset_cm'].count().reset_index()\n",
    "descr_stats2 = descr_stats.groupby(['condition', 'subj_nr']).unique_dot.count().reset_index()\n",
    "descr_stats2 = descr_stats2.groupby('condition').unique_dot.mean()\n",
    "print(f'Mean unique dots:\\n {descr_stats2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aefbbd",
   "metadata": {},
   "source": [
    "### How many calibration attempts for 9 and 13 conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfc028b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_calib_dots\n",
       "9     1.125\n",
       "13    1.125\n",
       "25    2.000\n",
       "Name: frequency, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_calib_attempts_df.groupby('num_calib_dots').frequency.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94b82a",
   "metadata": {},
   "source": [
    "### To do\n",
    "1. Summarize the incomplete datasets (failed calibration, other reasons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
